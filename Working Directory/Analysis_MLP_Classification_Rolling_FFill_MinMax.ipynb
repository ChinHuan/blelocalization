{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width: 98% !important }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 98% !important }<style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import *\n",
    "from models import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_file = \"../Data/pin.csv\"\n",
    "\n",
    "pin = read_pin(pin_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All beacons: ['0117C55D14E4']\n",
      "Selecting 0117C55D14E4\n"
     ]
    }
   ],
   "source": [
    "filename = \"../Data/rssi4.csv\"\n",
    "B1 = \"0117C55D14E4\"\n",
    "\n",
    "data = read_data(filename, B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[scanners] = minMaxScaling(data[scanners])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = train_validation_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C400A2E19293</th>\n",
       "      <th>CD4533FFC0E1</th>\n",
       "      <th>D2B6503554D7</th>\n",
       "      <th>DB8B36A69C56</th>\n",
       "      <th>DD697EA75B68</th>\n",
       "      <th>DF231643E227</th>\n",
       "      <th>E13B805C6CB0</th>\n",
       "      <th>E43355CA8B96</th>\n",
       "      <th>E6D9D20DD197</th>\n",
       "      <th>E8FD0B453DC4</th>\n",
       "      <th>E96AF2C858BA</th>\n",
       "      <th>EC72840D9AD3</th>\n",
       "      <th>F1307ECB3B90</th>\n",
       "      <th>F1EDAF28E08A</th>\n",
       "      <th>F69A86823B96</th>\n",
       "      <th>FB2EE01C18CE</th>\n",
       "      <th>FDAE5980F28C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V1_11</th>\n",
       "      <td>55</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>96</td>\n",
       "      <td>126</td>\n",
       "      <td>82</td>\n",
       "      <td>60</td>\n",
       "      <td>105</td>\n",
       "      <td>120</td>\n",
       "      <td>49</td>\n",
       "      <td>48</td>\n",
       "      <td>130</td>\n",
       "      <td>129</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_12</th>\n",
       "      <td>67</td>\n",
       "      <td>76</td>\n",
       "      <td>43</td>\n",
       "      <td>84</td>\n",
       "      <td>79</td>\n",
       "      <td>76</td>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "      <td>153</td>\n",
       "      <td>51</td>\n",
       "      <td>74</td>\n",
       "      <td>151</td>\n",
       "      <td>126</td>\n",
       "      <td>79</td>\n",
       "      <td>106</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_13</th>\n",
       "      <td>60</td>\n",
       "      <td>141</td>\n",
       "      <td>47</td>\n",
       "      <td>111</td>\n",
       "      <td>42</td>\n",
       "      <td>86</td>\n",
       "      <td>101</td>\n",
       "      <td>111</td>\n",
       "      <td>229</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>107</td>\n",
       "      <td>80</td>\n",
       "      <td>92</td>\n",
       "      <td>150</td>\n",
       "      <td>14</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_14</th>\n",
       "      <td>36</td>\n",
       "      <td>154</td>\n",
       "      <td>45</td>\n",
       "      <td>82</td>\n",
       "      <td>56</td>\n",
       "      <td>89</td>\n",
       "      <td>261</td>\n",
       "      <td>66</td>\n",
       "      <td>210</td>\n",
       "      <td>14</td>\n",
       "      <td>105</td>\n",
       "      <td>50</td>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>164</td>\n",
       "      <td>63</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_20</th>\n",
       "      <td>105</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "      <td>77</td>\n",
       "      <td>124</td>\n",
       "      <td>98</td>\n",
       "      <td>61</td>\n",
       "      <td>82</td>\n",
       "      <td>72</td>\n",
       "      <td>31</td>\n",
       "      <td>72</td>\n",
       "      <td>90</td>\n",
       "      <td>169</td>\n",
       "      <td>15</td>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_21</th>\n",
       "      <td>64</td>\n",
       "      <td>132</td>\n",
       "      <td>78</td>\n",
       "      <td>72</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>52</td>\n",
       "      <td>132</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>78</td>\n",
       "      <td>64</td>\n",
       "      <td>85</td>\n",
       "      <td>70</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_22</th>\n",
       "      <td>61</td>\n",
       "      <td>70</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>68</td>\n",
       "      <td>155</td>\n",
       "      <td>205</td>\n",
       "      <td>27</td>\n",
       "      <td>233</td>\n",
       "      <td>27</td>\n",
       "      <td>136</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>109</td>\n",
       "      <td>90</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_23</th>\n",
       "      <td>57</td>\n",
       "      <td>86</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>202</td>\n",
       "      <td>123</td>\n",
       "      <td>46</td>\n",
       "      <td>290</td>\n",
       "      <td>31</td>\n",
       "      <td>194</td>\n",
       "      <td>58</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_24</th>\n",
       "      <td>22</td>\n",
       "      <td>67</td>\n",
       "      <td>53</td>\n",
       "      <td>55</td>\n",
       "      <td>39</td>\n",
       "      <td>108</td>\n",
       "      <td>95</td>\n",
       "      <td>47</td>\n",
       "      <td>441</td>\n",
       "      <td>17</td>\n",
       "      <td>184</td>\n",
       "      <td>54</td>\n",
       "      <td>64</td>\n",
       "      <td>33</td>\n",
       "      <td>87</td>\n",
       "      <td>135</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_28</th>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>52</td>\n",
       "      <td>109</td>\n",
       "      <td>75</td>\n",
       "      <td>96</td>\n",
       "      <td>63</td>\n",
       "      <td>108</td>\n",
       "      <td>70</td>\n",
       "      <td>46</td>\n",
       "      <td>67</td>\n",
       "      <td>57</td>\n",
       "      <td>97</td>\n",
       "      <td>86</td>\n",
       "      <td>27</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_29</th>\n",
       "      <td>60</td>\n",
       "      <td>132</td>\n",
       "      <td>87</td>\n",
       "      <td>57</td>\n",
       "      <td>87</td>\n",
       "      <td>75</td>\n",
       "      <td>145</td>\n",
       "      <td>69</td>\n",
       "      <td>128</td>\n",
       "      <td>25</td>\n",
       "      <td>91</td>\n",
       "      <td>68</td>\n",
       "      <td>87</td>\n",
       "      <td>32</td>\n",
       "      <td>95</td>\n",
       "      <td>108</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_30</th>\n",
       "      <td>34</td>\n",
       "      <td>84</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>52</td>\n",
       "      <td>216</td>\n",
       "      <td>182</td>\n",
       "      <td>28</td>\n",
       "      <td>166</td>\n",
       "      <td>17</td>\n",
       "      <td>95</td>\n",
       "      <td>47</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>76</td>\n",
       "      <td>122</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_31</th>\n",
       "      <td>18</td>\n",
       "      <td>56</td>\n",
       "      <td>37</td>\n",
       "      <td>59</td>\n",
       "      <td>45</td>\n",
       "      <td>113</td>\n",
       "      <td>91</td>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "      <td>7</td>\n",
       "      <td>174</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>152</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1_32</th>\n",
       "      <td>30</td>\n",
       "      <td>82</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>129</td>\n",
       "      <td>126</td>\n",
       "      <td>41</td>\n",
       "      <td>311</td>\n",
       "      <td>21</td>\n",
       "      <td>199</td>\n",
       "      <td>62</td>\n",
       "      <td>38</td>\n",
       "      <td>18</td>\n",
       "      <td>68</td>\n",
       "      <td>138</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          C400A2E19293  CD4533FFC0E1  D2B6503554D7  DB8B36A69C56  \\\n",
       "location                                                           \n",
       "V1_11               55            71            68            96   \n",
       "V1_12               67            76            43            84   \n",
       "V1_13               60           141            47           111   \n",
       "V1_14               36           154            45            82   \n",
       "V1_20              105            92            94            77   \n",
       "V1_21               64           132            78            72   \n",
       "V1_22               61            70            58            62   \n",
       "V1_23               57            86            53            47   \n",
       "V1_24               22            67            53            55   \n",
       "V1_28               99            96            93            52   \n",
       "V1_29               60           132            87            57   \n",
       "V1_30               34            84            68            27   \n",
       "V1_31               18            56            37            59   \n",
       "V1_32               30            82            64            66   \n",
       "\n",
       "          DD697EA75B68  DF231643E227  E13B805C6CB0  E43355CA8B96  \\\n",
       "location                                                           \n",
       "V1_11              126            82            60           105   \n",
       "V1_12               79            76            56            58   \n",
       "V1_13               42            86           101           111   \n",
       "V1_14               56            89           261            66   \n",
       "V1_20              124            98            61            82   \n",
       "V1_21               90            90           120            52   \n",
       "V1_22               68           155           205            27   \n",
       "V1_23               35           202           123            46   \n",
       "V1_24               39           108            95            47   \n",
       "V1_28              109            75            96            63   \n",
       "V1_29               87            75           145            69   \n",
       "V1_30               52           216           182            28   \n",
       "V1_31               45           113            91            25   \n",
       "V1_32               65           129           126            41   \n",
       "\n",
       "          E6D9D20DD197  E8FD0B453DC4  E96AF2C858BA  EC72840D9AD3  \\\n",
       "location                                                           \n",
       "V1_11              120            49            48           130   \n",
       "V1_12              153            51            74           151   \n",
       "V1_13              229            13            33           107   \n",
       "V1_14              210            14           105            50   \n",
       "V1_20               72            31            72            90   \n",
       "V1_21              132            17            86            93   \n",
       "V1_22              233            27           136            57   \n",
       "V1_23              290            31           194            58   \n",
       "V1_24              441            17           184            54   \n",
       "V1_28              108            70            46            67   \n",
       "V1_29              128            25            91            68   \n",
       "V1_30              166            17            95            47   \n",
       "V1_31              150             7           174            51   \n",
       "V1_32              311            21           199            62   \n",
       "\n",
       "          F1307ECB3B90  F1EDAF28E08A  F69A86823B96  FB2EE01C18CE  FDAE5980F28C  \n",
       "location                                                                        \n",
       "V1_11              129            27            45            20            84  \n",
       "V1_12              126            79           106            56            71  \n",
       "V1_13               80            92           150            14            63  \n",
       "V1_14               78            83           164            63            86  \n",
       "V1_20              169            15            74            23            69  \n",
       "V1_21               78            64            85            70            62  \n",
       "V1_22               49            11           109            90            51  \n",
       "V1_23               79             9           107           116            61  \n",
       "V1_24               64            33            87           135            26  \n",
       "V1_28               57            97            86            27            64  \n",
       "V1_29               87            32            95           108            60  \n",
       "V1_30               46            12            76           122            63  \n",
       "V1_31               36             6            46           152            40  \n",
       "V1_32               38            18            68           138            93  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"location\")[scanners].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6034, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6035, 19)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sort_values(\"time\", inplace=True)\n",
    "validation.sort_values(\"time\", inplace=True)\n",
    "test.sort_values(\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rolled = train.groupby(\"location\").rolling(15, min_periods=1).mean().reset_index()\n",
    "validation_rolled = validation.groupby(\"location\").rolling(15, min_periods=1).mean().reset_index()\n",
    "test_rolled = test.groupby(\"location\").rolling(15, min_periods=1).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>level_1</th>\n",
       "      <th>C400A2E19293</th>\n",
       "      <th>CD4533FFC0E1</th>\n",
       "      <th>D2B6503554D7</th>\n",
       "      <th>DB8B36A69C56</th>\n",
       "      <th>DD697EA75B68</th>\n",
       "      <th>DF231643E227</th>\n",
       "      <th>E13B805C6CB0</th>\n",
       "      <th>E43355CA8B96</th>\n",
       "      <th>E6D9D20DD197</th>\n",
       "      <th>E8FD0B453DC4</th>\n",
       "      <th>E96AF2C858BA</th>\n",
       "      <th>EC72840D9AD3</th>\n",
       "      <th>F1307ECB3B90</th>\n",
       "      <th>F1EDAF28E08A</th>\n",
       "      <th>F69A86823B96</th>\n",
       "      <th>FB2EE01C18CE</th>\n",
       "      <th>FDAE5980F28C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1_11</td>\n",
       "      <td>24304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V1_11</td>\n",
       "      <td>24306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V1_11</td>\n",
       "      <td>24307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V1_11</td>\n",
       "      <td>24308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V1_11</td>\n",
       "      <td>24310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18097</th>\n",
       "      <td>V1_32</td>\n",
       "      <td>22547</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18098</th>\n",
       "      <td>V1_32</td>\n",
       "      <td>22549</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18099</th>\n",
       "      <td>V1_32</td>\n",
       "      <td>22551</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18100</th>\n",
       "      <td>V1_32</td>\n",
       "      <td>22552</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18101</th>\n",
       "      <td>V1_32</td>\n",
       "      <td>22554</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18102 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      location  level_1  C400A2E19293  CD4533FFC0E1  D2B6503554D7  \\\n",
       "0        V1_11    24304           NaN           NaN           NaN   \n",
       "1        V1_11    24306           NaN           NaN           NaN   \n",
       "2        V1_11    24307           NaN           NaN           NaN   \n",
       "3        V1_11    24308           NaN      0.333333           NaN   \n",
       "4        V1_11    24310           NaN      0.333333           NaN   \n",
       "...        ...      ...           ...           ...           ...   \n",
       "18097    V1_32    22547      0.183333      0.283333      0.333333   \n",
       "18098    V1_32    22549      0.183333      0.283333      0.333333   \n",
       "18099    V1_32    22551      0.183333      0.283333      0.333333   \n",
       "18100    V1_32    22552      0.183333      0.283333      0.333333   \n",
       "18101    V1_32    22554      0.183333      0.283333      0.333333   \n",
       "\n",
       "       DB8B36A69C56  DD697EA75B68  DF231643E227  E13B805C6CB0  E43355CA8B96  \\\n",
       "0              0.55           NaN           NaN           NaN           NaN   \n",
       "1              0.55      0.516667           NaN           NaN           NaN   \n",
       "2              0.55      0.516667           NaN      0.333333           NaN   \n",
       "3              0.55      0.516667           NaN      0.333333           NaN   \n",
       "4              0.55      0.516667          0.15      0.333333           NaN   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "18097           NaN           NaN          0.55      0.433333           NaN   \n",
       "18098           NaN      0.233333          0.55      0.433333           NaN   \n",
       "18099           NaN      0.233333          0.55      0.433333           NaN   \n",
       "18100           NaN      0.233333           NaN      0.433333           NaN   \n",
       "18101           NaN      0.233333           NaN      0.400000           NaN   \n",
       "\n",
       "       E6D9D20DD197  E8FD0B453DC4  E96AF2C858BA  EC72840D9AD3  F1307ECB3B90  \\\n",
       "0               NaN           NaN           NaN           NaN           NaN   \n",
       "1               NaN           NaN           NaN           NaN           NaN   \n",
       "2               NaN           NaN           NaN           NaN           NaN   \n",
       "3               NaN           NaN           NaN           NaN           NaN   \n",
       "4               NaN           NaN      0.100000           NaN           NaN   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "18097      0.516667           NaN      0.700000          0.25           NaN   \n",
       "18098      0.500000           NaN      0.700000          0.25           0.3   \n",
       "18099      0.505556           NaN      0.700000           NaN           0.3   \n",
       "18100      0.505556           NaN      0.679167           NaN           0.3   \n",
       "18101      0.516667           NaN      0.679167           NaN           0.3   \n",
       "\n",
       "       F1EDAF28E08A  F69A86823B96  FB2EE01C18CE  FDAE5980F28C  \n",
       "0               NaN           NaN           NaN           NaN  \n",
       "1               NaN           NaN           NaN           NaN  \n",
       "2               NaN           NaN           NaN           NaN  \n",
       "3               NaN           NaN           NaN           NaN  \n",
       "4               NaN           NaN           NaN           NaN  \n",
       "...             ...           ...           ...           ...  \n",
       "18097           NaN      0.258333           NaN           NaN  \n",
       "18098           NaN      0.258333           NaN           NaN  \n",
       "18099           NaN      0.258333           NaN           NaN  \n",
       "18100           NaN      0.258333           NaN           NaN  \n",
       "18101           NaN      0.258333           NaN           NaN  \n",
       "\n",
       "[18102 rows x 19 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rolled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "1. Forward fill\n",
    "2. Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed = train.set_index(\"location\").groupby(\"location\").ffill()\n",
    "train_imputed.fillna(0, inplace=True)\n",
    "train_imputed.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_imputed = validation.set_index(\"location\").groupby(\"location\").ffill()\n",
    "validation_imputed.fillna(0, inplace=True)\n",
    "validation_imputed.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imputed = test.set_index(\"location\").groupby(\"location\").ffill()\n",
    "test_imputed.fillna(0, inplace=True)\n",
    "test_imputed.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Label Encoding for Each Fingerprint Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_imputed[scanners].values, train_imputed[\"location\"].values\n",
    "X_validation, y_validation = validation_imputed[scanners].values, validation_imputed[\"location\"].values\n",
    "X_test, y_test = test_imputed[scanners].values, test_imputed[\"location\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_validation = enc.transform(y_validation)\n",
    "y_test = enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18102,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                462       \n",
      "=================================================================\n",
      "Total params: 3,694\n",
      "Trainable params: 3,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18102 samples, validate on 6034 samples\n",
      "Epoch 1/2000\n",
      "18102/18102 [==============================] - 1s 66us/sample - loss: 1.8213 - accuracy: 0.3941 - val_loss: 1.0980 - val_accuracy: 0.7547\n",
      "Epoch 2/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.9974 - accuracy: 0.6687 - val_loss: 0.7134 - val_accuracy: 0.8384\n",
      "Epoch 3/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.7399 - accuracy: 0.7756 - val_loss: 0.5837 - val_accuracy: 0.8580\n",
      "Epoch 4/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.6211 - accuracy: 0.8234 - val_loss: 0.5182 - val_accuracy: 0.8752\n",
      "Epoch 5/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.5510 - accuracy: 0.8524 - val_loss: 0.4917 - val_accuracy: 0.8823\n",
      "Epoch 6/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.4980 - accuracy: 0.8705 - val_loss: 0.4695 - val_accuracy: 0.8888\n",
      "Epoch 7/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.4624 - accuracy: 0.8834 - val_loss: 0.4443 - val_accuracy: 0.9011\n",
      "Epoch 8/2000\n",
      "18102/18102 [==============================] - 1s 44us/sample - loss: 0.4340 - accuracy: 0.8938 - val_loss: 0.4299 - val_accuracy: 0.9070\n",
      "Epoch 9/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.4132 - accuracy: 0.8995 - val_loss: 0.4209 - val_accuracy: 0.9166\n",
      "Epoch 10/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.3896 - accuracy: 0.9105 - val_loss: 0.4111 - val_accuracy: 0.9133\n",
      "Epoch 11/2000\n",
      "18102/18102 [==============================] - 1s 38us/sample - loss: 0.3774 - accuracy: 0.9105 - val_loss: 0.4119 - val_accuracy: 0.9102\n",
      "Epoch 12/2000\n",
      "18102/18102 [==============================] - 1s 45us/sample - loss: 0.3592 - accuracy: 0.9201 - val_loss: 0.3896 - val_accuracy: 0.9214\n",
      "Epoch 13/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.3407 - accuracy: 0.9267 - val_loss: 0.3752 - val_accuracy: 0.9277\n",
      "Epoch 14/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.3282 - accuracy: 0.9284 - val_loss: 0.3706 - val_accuracy: 0.9286\n",
      "Epoch 15/2000\n",
      "18102/18102 [==============================] - 1s 41us/sample - loss: 0.3163 - accuracy: 0.9316 - val_loss: 0.3762 - val_accuracy: 0.9251\n",
      "Epoch 16/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.3033 - accuracy: 0.9379 - val_loss: 0.3678 - val_accuracy: 0.9281\n",
      "Epoch 17/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.3024 - accuracy: 0.9388 - val_loss: 0.3596 - val_accuracy: 0.9301\n",
      "Epoch 18/2000\n",
      "18102/18102 [==============================] - 1s 36us/sample - loss: 0.2873 - accuracy: 0.9418 - val_loss: 0.3651 - val_accuracy: 0.9282\n",
      "Epoch 19/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2807 - accuracy: 0.9452 - val_loss: 0.3474 - val_accuracy: 0.9349\n",
      "Epoch 20/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2757 - accuracy: 0.9441 - val_loss: 0.3485 - val_accuracy: 0.9369\n",
      "Epoch 21/2000\n",
      "18102/18102 [==============================] - 1s 40us/sample - loss: 0.2660 - accuracy: 0.9465 - val_loss: 0.3534 - val_accuracy: 0.9324\n",
      "Epoch 22/2000\n",
      "18102/18102 [==============================] - 1s 41us/sample - loss: 0.2663 - accuracy: 0.9459 - val_loss: 0.3434 - val_accuracy: 0.9369\n",
      "Epoch 23/2000\n",
      "18102/18102 [==============================] - 1s 35us/sample - loss: 0.2620 - accuracy: 0.9464 - val_loss: 0.3388 - val_accuracy: 0.9370\n",
      "Epoch 24/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2541 - accuracy: 0.9505 - val_loss: 0.3416 - val_accuracy: 0.9370\n",
      "Epoch 25/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2459 - accuracy: 0.9532 - val_loss: 0.3380 - val_accuracy: 0.9380\n",
      "Epoch 26/2000\n",
      "18102/18102 [==============================] - 1s 36us/sample - loss: 0.2455 - accuracy: 0.9525 - val_loss: 0.3402 - val_accuracy: 0.9388\n",
      "Epoch 27/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.2343 - accuracy: 0.9556 - val_loss: 0.3330 - val_accuracy: 0.9415\n",
      "Epoch 28/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2356 - accuracy: 0.9564 - val_loss: 0.3394 - val_accuracy: 0.9375\n",
      "Epoch 29/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2293 - accuracy: 0.9558 - val_loss: 0.3351 - val_accuracy: 0.9367\n",
      "Epoch 30/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.2269 - accuracy: 0.9564 - val_loss: 0.3279 - val_accuracy: 0.9451\n",
      "Epoch 31/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2224 - accuracy: 0.9585 - val_loss: 0.3223 - val_accuracy: 0.9397\n",
      "Epoch 32/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2177 - accuracy: 0.9609 - val_loss: 0.3241 - val_accuracy: 0.9408\n",
      "Epoch 33/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2160 - accuracy: 0.9601 - val_loss: 0.3471 - val_accuracy: 0.9321\n",
      "Epoch 34/2000\n",
      "18102/18102 [==============================] - 1s 36us/sample - loss: 0.2137 - accuracy: 0.9597 - val_loss: 0.3314 - val_accuracy: 0.9407\n",
      "Epoch 35/2000\n",
      "18102/18102 [==============================] - 1s 45us/sample - loss: 0.2064 - accuracy: 0.9637 - val_loss: 0.3197 - val_accuracy: 0.9428\n",
      "Epoch 36/2000\n",
      "18102/18102 [==============================] - 1s 37us/sample - loss: 0.2089 - accuracy: 0.9607 - val_loss: 0.3302 - val_accuracy: 0.9430\n",
      "Epoch 37/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2062 - accuracy: 0.9604 - val_loss: 0.3353 - val_accuracy: 0.9365\n",
      "Epoch 38/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.2040 - accuracy: 0.9629 - val_loss: 0.3142 - val_accuracy: 0.9441\n",
      "Epoch 39/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1993 - accuracy: 0.9640 - val_loss: 0.3148 - val_accuracy: 0.9471\n",
      "Epoch 40/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1977 - accuracy: 0.9628 - val_loss: 0.3164 - val_accuracy: 0.9441\n",
      "Epoch 41/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1965 - accuracy: 0.9637 - val_loss: 0.3143 - val_accuracy: 0.9410\n",
      "Epoch 42/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1900 - accuracy: 0.9661 - val_loss: 0.3126 - val_accuracy: 0.9471\n",
      "Epoch 43/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1908 - accuracy: 0.9652 - val_loss: 0.3085 - val_accuracy: 0.9440\n",
      "Epoch 44/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1941 - accuracy: 0.9636 - val_loss: 0.3097 - val_accuracy: 0.9441\n",
      "Epoch 45/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1901 - accuracy: 0.9659 - val_loss: 0.3024 - val_accuracy: 0.9465\n",
      "Epoch 46/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1901 - accuracy: 0.9649 - val_loss: 0.3081 - val_accuracy: 0.9476\n",
      "Epoch 47/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1855 - accuracy: 0.9688 - val_loss: 0.3053 - val_accuracy: 0.9448\n",
      "Epoch 48/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1818 - accuracy: 0.9677 - val_loss: 0.3098 - val_accuracy: 0.9432\n",
      "Epoch 49/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1786 - accuracy: 0.9690 - val_loss: 0.3157 - val_accuracy: 0.9417\n",
      "Epoch 50/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1794 - accuracy: 0.9677 - val_loss: 0.3113 - val_accuracy: 0.9445\n",
      "Epoch 51/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1739 - accuracy: 0.9687 - val_loss: 0.3054 - val_accuracy: 0.9488\n",
      "Epoch 52/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1775 - accuracy: 0.9683 - val_loss: 0.3123 - val_accuracy: 0.9441\n",
      "Epoch 53/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1737 - accuracy: 0.9695 - val_loss: 0.3102 - val_accuracy: 0.9432\n",
      "Epoch 54/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1716 - accuracy: 0.9703 - val_loss: 0.3224 - val_accuracy: 0.9398\n",
      "Epoch 55/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1726 - accuracy: 0.9694 - val_loss: 0.3027 - val_accuracy: 0.9491\n",
      "Epoch 56/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1711 - accuracy: 0.9704 - val_loss: 0.3015 - val_accuracy: 0.9478\n",
      "Epoch 57/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1715 - accuracy: 0.9675 - val_loss: 0.2974 - val_accuracy: 0.9456\n",
      "Epoch 58/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1639 - accuracy: 0.9710 - val_loss: 0.3084 - val_accuracy: 0.9456\n",
      "Epoch 59/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1667 - accuracy: 0.9693 - val_loss: 0.2931 - val_accuracy: 0.9485\n",
      "Epoch 60/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1639 - accuracy: 0.9711 - val_loss: 0.2996 - val_accuracy: 0.9463\n",
      "Epoch 61/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1654 - accuracy: 0.9699 - val_loss: 0.2919 - val_accuracy: 0.9473\n",
      "Epoch 62/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.1625 - accuracy: 0.9711 - val_loss: 0.3041 - val_accuracy: 0.9415\n",
      "Epoch 63/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1609 - accuracy: 0.9724 - val_loss: 0.3034 - val_accuracy: 0.9456\n",
      "Epoch 64/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1614 - accuracy: 0.9718 - val_loss: 0.2902 - val_accuracy: 0.9495\n",
      "Epoch 65/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1611 - accuracy: 0.9720 - val_loss: 0.2958 - val_accuracy: 0.9478\n",
      "Epoch 66/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1583 - accuracy: 0.9727 - val_loss: 0.2905 - val_accuracy: 0.9514\n",
      "Epoch 67/2000\n",
      "18102/18102 [==============================] - 1s 36us/sample - loss: 0.1565 - accuracy: 0.9727 - val_loss: 0.2954 - val_accuracy: 0.9461\n",
      "Epoch 68/2000\n",
      "18102/18102 [==============================] - 1s 37us/sample - loss: 0.1597 - accuracy: 0.9710 - val_loss: 0.2888 - val_accuracy: 0.9478\n",
      "Epoch 69/2000\n",
      "18102/18102 [==============================] - 1s 35us/sample - loss: 0.1575 - accuracy: 0.9719 - val_loss: 0.2970 - val_accuracy: 0.9448\n",
      "Epoch 70/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1536 - accuracy: 0.9727 - val_loss: 0.2986 - val_accuracy: 0.9443\n",
      "Epoch 71/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1550 - accuracy: 0.9735 - val_loss: 0.2853 - val_accuracy: 0.9463\n",
      "Epoch 72/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1502 - accuracy: 0.9745 - val_loss: 0.3007 - val_accuracy: 0.9495\n",
      "Epoch 73/2000\n",
      "18102/18102 [==============================] - 1s 36us/sample - loss: 0.1507 - accuracy: 0.9741 - val_loss: 0.3036 - val_accuracy: 0.9441\n",
      "Epoch 74/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1516 - accuracy: 0.9723 - val_loss: 0.2939 - val_accuracy: 0.9460\n",
      "Epoch 75/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1460 - accuracy: 0.9733 - val_loss: 0.2968 - val_accuracy: 0.9465\n",
      "Epoch 76/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1468 - accuracy: 0.9743 - val_loss: 0.2891 - val_accuracy: 0.9516\n",
      "Epoch 77/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1473 - accuracy: 0.9736 - val_loss: 0.3194 - val_accuracy: 0.9395\n",
      "Epoch 78/2000\n",
      "18102/18102 [==============================] - 1s 35us/sample - loss: 0.1473 - accuracy: 0.9746 - val_loss: 0.2921 - val_accuracy: 0.9478\n",
      "Epoch 79/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1501 - accuracy: 0.9735 - val_loss: 0.2883 - val_accuracy: 0.9486\n",
      "Epoch 80/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1459 - accuracy: 0.9739 - val_loss: 0.2888 - val_accuracy: 0.9509\n",
      "Epoch 81/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1451 - accuracy: 0.9753 - val_loss: 0.2991 - val_accuracy: 0.9456\n",
      "Epoch 82/2000\n",
      "18102/18102 [==============================] - 1s 36us/sample - loss: 0.1424 - accuracy: 0.9755 - val_loss: 0.2969 - val_accuracy: 0.9463\n",
      "Epoch 83/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1439 - accuracy: 0.9749 - val_loss: 0.3070 - val_accuracy: 0.9423\n",
      "Epoch 84/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1397 - accuracy: 0.9764 - val_loss: 0.2984 - val_accuracy: 0.9456\n",
      "Epoch 85/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1408 - accuracy: 0.9747 - val_loss: 0.2794 - val_accuracy: 0.9504\n",
      "Epoch 86/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1394 - accuracy: 0.9760 - val_loss: 0.2930 - val_accuracy: 0.9465\n",
      "Epoch 87/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1384 - accuracy: 0.9771 - val_loss: 0.2923 - val_accuracy: 0.9496\n",
      "Epoch 88/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1380 - accuracy: 0.9771 - val_loss: 0.2918 - val_accuracy: 0.9495\n",
      "Epoch 89/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.1401 - accuracy: 0.9759 - val_loss: 0.2860 - val_accuracy: 0.9500\n",
      "Epoch 90/2000\n",
      "18102/18102 [==============================] - 1s 36us/sample - loss: 0.1365 - accuracy: 0.9767 - val_loss: 0.3232 - val_accuracy: 0.9402\n",
      "Epoch 91/2000\n",
      "18102/18102 [==============================] - 1s 38us/sample - loss: 0.1389 - accuracy: 0.9760 - val_loss: 0.2899 - val_accuracy: 0.9491\n",
      "Epoch 92/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.1363 - accuracy: 0.9770 - val_loss: 0.2874 - val_accuracy: 0.9471\n",
      "Epoch 93/2000\n",
      "18102/18102 [==============================] - 1s 37us/sample - loss: 0.1350 - accuracy: 0.9762 - val_loss: 0.2993 - val_accuracy: 0.9468\n",
      "Epoch 94/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1354 - accuracy: 0.9761 - val_loss: 0.2834 - val_accuracy: 0.9493\n",
      "Epoch 95/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1357 - accuracy: 0.9766 - val_loss: 0.3029 - val_accuracy: 0.9415\n",
      "Epoch 96/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.1339 - accuracy: 0.9773 - val_loss: 0.2909 - val_accuracy: 0.9493\n",
      "Epoch 97/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1364 - accuracy: 0.9746 - val_loss: 0.2912 - val_accuracy: 0.9468\n",
      "Epoch 98/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1301 - accuracy: 0.9776 - val_loss: 0.3096 - val_accuracy: 0.9437\n",
      "Epoch 99/2000\n",
      "18102/18102 [==============================] - 1s 37us/sample - loss: 0.1294 - accuracy: 0.9785 - val_loss: 0.3067 - val_accuracy: 0.9430\n",
      "Epoch 100/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.1294 - accuracy: 0.9786 - val_loss: 0.2887 - val_accuracy: 0.9490\n",
      "Epoch 101/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1314 - accuracy: 0.9773 - val_loss: 0.2869 - val_accuracy: 0.9504\n",
      "Epoch 102/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1293 - accuracy: 0.9781 - val_loss: 0.3047 - val_accuracy: 0.9441\n",
      "Epoch 103/2000\n",
      "18102/18102 [==============================] - 1s 37us/sample - loss: 0.1286 - accuracy: 0.9791 - val_loss: 0.2921 - val_accuracy: 0.9490\n",
      "Epoch 104/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1301 - accuracy: 0.9771 - val_loss: 0.2970 - val_accuracy: 0.9495\n",
      "Epoch 105/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1286 - accuracy: 0.9772 - val_loss: 0.3027 - val_accuracy: 0.9417\n",
      "Epoch 106/2000\n",
      "18102/18102 [==============================] - 1s 35us/sample - loss: 0.1289 - accuracy: 0.9780 - val_loss: 0.2919 - val_accuracy: 0.9475\n",
      "Epoch 107/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1291 - accuracy: 0.9770 - val_loss: 0.3036 - val_accuracy: 0.9443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1296 - accuracy: 0.9777 - val_loss: 0.2940 - val_accuracy: 0.9518\n",
      "Epoch 109/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1266 - accuracy: 0.9785 - val_loss: 0.2923 - val_accuracy: 0.9546\n",
      "Epoch 110/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1283 - accuracy: 0.9775 - val_loss: 0.2910 - val_accuracy: 0.9514\n",
      "Epoch 111/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1244 - accuracy: 0.9798 - val_loss: 0.2838 - val_accuracy: 0.9513\n",
      "Epoch 112/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1243 - accuracy: 0.9798 - val_loss: 0.2942 - val_accuracy: 0.9488\n",
      "Epoch 113/2000\n",
      "18102/18102 [==============================] - 1s 35us/sample - loss: 0.1238 - accuracy: 0.9782 - val_loss: 0.2990 - val_accuracy: 0.9448\n",
      "Epoch 114/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1246 - accuracy: 0.9783 - val_loss: 0.2950 - val_accuracy: 0.9508\n",
      "Epoch 115/2000\n",
      "18102/18102 [==============================] - 1s 38us/sample - loss: 0.1233 - accuracy: 0.9792 - val_loss: 0.3093 - val_accuracy: 0.9435\n",
      "Epoch 116/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1247 - accuracy: 0.9786 - val_loss: 0.2952 - val_accuracy: 0.9503\n",
      "Epoch 117/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1243 - accuracy: 0.9780 - val_loss: 0.2995 - val_accuracy: 0.9473\n",
      "Epoch 118/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.1216 - accuracy: 0.9788 - val_loss: 0.3007 - val_accuracy: 0.9466\n",
      "Epoch 119/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1211 - accuracy: 0.9784 - val_loss: 0.2970 - val_accuracy: 0.9508\n",
      "Epoch 120/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1217 - accuracy: 0.9786 - val_loss: 0.2952 - val_accuracy: 0.9498\n",
      "Epoch 121/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1226 - accuracy: 0.9788 - val_loss: 0.3017 - val_accuracy: 0.9500\n",
      "Epoch 122/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1210 - accuracy: 0.9792 - val_loss: 0.3084 - val_accuracy: 0.9432\n",
      "Epoch 123/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1224 - accuracy: 0.9780 - val_loss: 0.3038 - val_accuracy: 0.9501\n",
      "Epoch 124/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1216 - accuracy: 0.9785 - val_loss: 0.2972 - val_accuracy: 0.9501\n",
      "Epoch 125/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1205 - accuracy: 0.9789 - val_loss: 0.2876 - val_accuracy: 0.9495\n",
      "Epoch 126/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1187 - accuracy: 0.9784 - val_loss: 0.2935 - val_accuracy: 0.9508\n",
      "Epoch 127/2000\n",
      "18102/18102 [==============================] - 1s 37us/sample - loss: 0.1167 - accuracy: 0.9793 - val_loss: 0.2899 - val_accuracy: 0.9483\n",
      "Epoch 128/2000\n",
      "18102/18102 [==============================] - 1s 43us/sample - loss: 0.1166 - accuracy: 0.9802 - val_loss: 0.2870 - val_accuracy: 0.9539\n",
      "Epoch 129/2000\n",
      "18102/18102 [==============================] - 1s 40us/sample - loss: 0.1167 - accuracy: 0.9798 - val_loss: 0.2820 - val_accuracy: 0.9558\n",
      "Epoch 130/2000\n",
      "18102/18102 [==============================] - 1s 43us/sample - loss: 0.1186 - accuracy: 0.9795 - val_loss: 0.3056 - val_accuracy: 0.9460\n",
      "Epoch 131/2000\n",
      "18102/18102 [==============================] - 1s 40us/sample - loss: 0.1148 - accuracy: 0.9808 - val_loss: 0.2960 - val_accuracy: 0.9506\n",
      "Epoch 132/2000\n",
      "18102/18102 [==============================] - 1s 39us/sample - loss: 0.1170 - accuracy: 0.9796 - val_loss: 0.2866 - val_accuracy: 0.9514\n",
      "Epoch 133/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.1163 - accuracy: 0.9803 - val_loss: 0.2882 - val_accuracy: 0.9498\n",
      "Epoch 134/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1175 - accuracy: 0.9791 - val_loss: 0.3032 - val_accuracy: 0.9490\n",
      "Epoch 135/2000\n",
      "18102/18102 [==============================] - 1s 35us/sample - loss: 0.1152 - accuracy: 0.9803 - val_loss: 0.2916 - val_accuracy: 0.9438\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.show_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6034/1 - 0s - loss: 0.1839 - accuracy: 0.9438\n",
      "Validation loss: 0.29162647530441455\n",
      "Accuracy: 0.9438184\n"
     ]
    }
   ],
   "source": [
    "model.model_validation(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6035/1 - 0s - loss: 0.1584 - accuracy: 0.9519\n",
      "Test loss: 0.26624822909970763\n",
      "Accuracy 0.951947\n"
     ]
    }
   ],
   "source": [
    "model.model_testing(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1810 samples, validate on 603 samples\n",
      "Epoch 1/2000\n",
      "1810/1810 [==============================] - 0s 204us/sample - loss: 2.5812 - accuracy: 0.1503 - val_loss: 2.5288 - val_accuracy: 0.2023\n",
      "Epoch 2/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 2.4262 - accuracy: 0.2652 - val_loss: 2.4050 - val_accuracy: 0.2471\n",
      "Epoch 3/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 2.2732 - accuracy: 0.3530 - val_loss: 2.2704 - val_accuracy: 0.3134\n",
      "Epoch 4/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 2.1145 - accuracy: 0.4077 - val_loss: 2.1344 - val_accuracy: 0.3964\n",
      "Epoch 5/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 1.9610 - accuracy: 0.4669 - val_loss: 2.0124 - val_accuracy: 0.4959\n",
      "Epoch 6/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 1.8278 - accuracy: 0.5282 - val_loss: 1.9093 - val_accuracy: 0.5141\n",
      "Epoch 7/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 1.7095 - accuracy: 0.5409 - val_loss: 1.8221 - val_accuracy: 0.5456\n",
      "Epoch 8/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 1.5997 - accuracy: 0.5768 - val_loss: 1.7392 - val_accuracy: 0.5655\n",
      "Epoch 9/2000\n",
      "1810/1810 [==============================] - 0s 35us/sample - loss: 1.5016 - accuracy: 0.6083 - val_loss: 1.6706 - val_accuracy: 0.5655\n",
      "Epoch 10/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 1.4124 - accuracy: 0.6315 - val_loss: 1.6041 - val_accuracy: 0.5987\n",
      "Epoch 11/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 1.3154 - accuracy: 0.6685 - val_loss: 1.5581 - val_accuracy: 0.5887\n",
      "Epoch 12/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 1.2690 - accuracy: 0.6867 - val_loss: 1.5138 - val_accuracy: 0.5920\n",
      "Epoch 13/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 1.2019 - accuracy: 0.7000 - val_loss: 1.4668 - val_accuracy: 0.6285\n",
      "Epoch 14/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 1.1418 - accuracy: 0.7182 - val_loss: 1.4322 - val_accuracy: 0.6269\n",
      "Epoch 15/2000\n",
      "1810/1810 [==============================] - 0s 34us/sample - loss: 1.0761 - accuracy: 0.7365 - val_loss: 1.4056 - val_accuracy: 0.6285\n",
      "Epoch 16/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 1.0082 - accuracy: 0.7652 - val_loss: 1.3766 - val_accuracy: 0.6153\n",
      "Epoch 17/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.9901 - accuracy: 0.7569 - val_loss: 1.3539 - val_accuracy: 0.6086\n",
      "Epoch 18/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.9205 - accuracy: 0.7928 - val_loss: 1.3561 - val_accuracy: 0.5887\n",
      "Epoch 19/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.8716 - accuracy: 0.8044 - val_loss: 1.3396 - val_accuracy: 0.5987\n",
      "Epoch 20/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.8473 - accuracy: 0.8166 - val_loss: 1.3128 - val_accuracy: 0.5937\n",
      "Epoch 21/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.8134 - accuracy: 0.8099 - val_loss: 1.2923 - val_accuracy: 0.5987\n",
      "Epoch 22/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.7810 - accuracy: 0.8381 - val_loss: 1.3055 - val_accuracy: 0.5672\n",
      "Epoch 23/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.7568 - accuracy: 0.8298 - val_loss: 1.2907 - val_accuracy: 0.5854\n",
      "Epoch 24/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.7418 - accuracy: 0.8320 - val_loss: 1.2910 - val_accuracy: 0.5738\n",
      "Epoch 25/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.7080 - accuracy: 0.8569 - val_loss: 1.2834 - val_accuracy: 0.5721\n",
      "Epoch 26/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.6816 - accuracy: 0.8569 - val_loss: 1.2899 - val_accuracy: 0.5572\n",
      "Epoch 27/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.6679 - accuracy: 0.8508 - val_loss: 1.2859 - val_accuracy: 0.5556\n",
      "Epoch 28/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.6421 - accuracy: 0.8575 - val_loss: 1.2939 - val_accuracy: 0.5522\n",
      "Epoch 29/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.6160 - accuracy: 0.8724 - val_loss: 1.3047 - val_accuracy: 0.5589\n",
      "Epoch 30/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.6017 - accuracy: 0.8735 - val_loss: 1.2957 - val_accuracy: 0.5522\n",
      "Epoch 31/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.5762 - accuracy: 0.8829 - val_loss: 1.3009 - val_accuracy: 0.5622\n",
      "Epoch 32/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.5913 - accuracy: 0.8624 - val_loss: 1.2922 - val_accuracy: 0.5572\n",
      "Epoch 33/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.5645 - accuracy: 0.8895 - val_loss: 1.3167 - val_accuracy: 0.5456\n",
      "Epoch 34/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.5536 - accuracy: 0.8829 - val_loss: 1.3334 - val_accuracy: 0.5390\n",
      "Epoch 35/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.5272 - accuracy: 0.8917 - val_loss: 1.2993 - val_accuracy: 0.5605\n",
      "Epoch 36/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.5298 - accuracy: 0.8895 - val_loss: 1.3359 - val_accuracy: 0.5522\n",
      "Epoch 37/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.5159 - accuracy: 0.8890 - val_loss: 1.3439 - val_accuracy: 0.5638\n",
      "Epoch 38/2000\n",
      "1810/1810 [==============================] - 0s 38us/sample - loss: 0.4987 - accuracy: 0.8923 - val_loss: 1.3413 - val_accuracy: 0.5423\n",
      "Epoch 39/2000\n",
      "1810/1810 [==============================] - 0s 44us/sample - loss: 0.5034 - accuracy: 0.8978 - val_loss: 1.3541 - val_accuracy: 0.5456\n",
      "Epoch 40/2000\n",
      "1810/1810 [==============================] - 0s 46us/sample - loss: 0.4848 - accuracy: 0.8961 - val_loss: 1.3616 - val_accuracy: 0.5556\n",
      "Epoch 41/2000\n",
      "1810/1810 [==============================] - 0s 45us/sample - loss: 0.4667 - accuracy: 0.9022 - val_loss: 1.3644 - val_accuracy: 0.5522\n",
      "Epoch 42/2000\n",
      "1810/1810 [==============================] - 0s 42us/sample - loss: 0.4689 - accuracy: 0.9105 - val_loss: 1.3956 - val_accuracy: 0.5489\n",
      "Epoch 43/2000\n",
      "1810/1810 [==============================] - 0s 37us/sample - loss: 0.4613 - accuracy: 0.9039 - val_loss: 1.3903 - val_accuracy: 0.5390\n",
      "Epoch 44/2000\n",
      "1810/1810 [==============================] - 0s 34us/sample - loss: 0.4617 - accuracy: 0.9006 - val_loss: 1.3934 - val_accuracy: 0.5556\n",
      "Epoch 45/2000\n",
      "1810/1810 [==============================] - 0s 33us/sample - loss: 0.4411 - accuracy: 0.9122 - val_loss: 1.3998 - val_accuracy: 0.5390\n",
      "Epoch 46/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.4468 - accuracy: 0.9116 - val_loss: 1.4124 - val_accuracy: 0.5439\n",
      "Epoch 47/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.4332 - accuracy: 0.9193 - val_loss: 1.4206 - val_accuracy: 0.5257\n",
      "Epoch 48/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.4235 - accuracy: 0.9133 - val_loss: 1.4293 - val_accuracy: 0.5456\n",
      "Epoch 49/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.4177 - accuracy: 0.9188 - val_loss: 1.4250 - val_accuracy: 0.5622\n",
      "Epoch 50/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.4250 - accuracy: 0.9133 - val_loss: 1.4448 - val_accuracy: 0.5423\n",
      "Epoch 51/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.4036 - accuracy: 0.9199 - val_loss: 1.4550 - val_accuracy: 0.5406\n",
      "Epoch 52/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.4125 - accuracy: 0.9088 - val_loss: 1.4694 - val_accuracy: 0.5406\n",
      "Epoch 53/2000\n",
      "1810/1810 [==============================] - 0s 33us/sample - loss: 0.4090 - accuracy: 0.9149 - val_loss: 1.4720 - val_accuracy: 0.5489\n",
      "Epoch 54/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.3918 - accuracy: 0.9254 - val_loss: 1.4719 - val_accuracy: 0.5522\n",
      "Epoch 55/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3991 - accuracy: 0.9138 - val_loss: 1.4923 - val_accuracy: 0.5456\n",
      "Epoch 56/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.3781 - accuracy: 0.9282 - val_loss: 1.4831 - val_accuracy: 0.5522\n",
      "Epoch 57/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3850 - accuracy: 0.9227 - val_loss: 1.4950 - val_accuracy: 0.5340\n",
      "Epoch 58/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3796 - accuracy: 0.9271 - val_loss: 1.4952 - val_accuracy: 0.5456\n",
      "Epoch 59/2000\n",
      "1810/1810 [==============================] - 0s 34us/sample - loss: 0.3681 - accuracy: 0.9287 - val_loss: 1.5052 - val_accuracy: 0.5489\n",
      "Epoch 60/2000\n",
      "1810/1810 [==============================] - 0s 34us/sample - loss: 0.3578 - accuracy: 0.9315 - val_loss: 1.5451 - val_accuracy: 0.5357\n",
      "Epoch 61/2000\n",
      "1810/1810 [==============================] - 0s 34us/sample - loss: 0.3707 - accuracy: 0.9243 - val_loss: 1.5127 - val_accuracy: 0.5357\n",
      "Epoch 62/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3713 - accuracy: 0.9221 - val_loss: 1.5094 - val_accuracy: 0.5456\n",
      "Epoch 63/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3645 - accuracy: 0.9199 - val_loss: 1.5176 - val_accuracy: 0.5373\n",
      "Epoch 64/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3580 - accuracy: 0.9276 - val_loss: 1.5798 - val_accuracy: 0.5274\n",
      "Epoch 65/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.3503 - accuracy: 0.9365 - val_loss: 1.5709 - val_accuracy: 0.5274\n",
      "Epoch 66/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3507 - accuracy: 0.9337 - val_loss: 1.5565 - val_accuracy: 0.5373\n",
      "Epoch 67/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3621 - accuracy: 0.9326 - val_loss: 1.5676 - val_accuracy: 0.5357\n",
      "Epoch 68/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.3416 - accuracy: 0.9508 - val_loss: 1.5814 - val_accuracy: 0.5406\n",
      "Epoch 69/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.3502 - accuracy: 0.9381 - val_loss: 1.5707 - val_accuracy: 0.5390\n",
      "Epoch 70/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3537 - accuracy: 0.9331 - val_loss: 1.6017 - val_accuracy: 0.5307\n",
      "Epoch 71/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.3413 - accuracy: 0.9309 - val_loss: 1.5820 - val_accuracy: 0.5406\n",
      "Epoch 72/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.3452 - accuracy: 0.9365 - val_loss: 1.6101 - val_accuracy: 0.5240\n",
      "Epoch 73/2000\n",
      "1810/1810 [==============================] - 0s 32us/sample - loss: 0.3390 - accuracy: 0.9354 - val_loss: 1.6308 - val_accuracy: 0.5274\n",
      "Epoch 74/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3313 - accuracy: 0.9326 - val_loss: 1.6489 - val_accuracy: 0.5240\n",
      "Epoch 75/2000\n",
      "1810/1810 [==============================] - 0s 31us/sample - loss: 0.3289 - accuracy: 0.9337 - val_loss: 1.6099 - val_accuracy: 0.5340\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3620 samples, validate on 1207 samples\n",
      "Epoch 1/2000\n",
      "3620/3620 [==============================] - 0s 138us/sample - loss: 2.5285 - accuracy: 0.1829 - val_loss: 2.4250 - val_accuracy: 0.2792\n",
      "Epoch 2/2000\n",
      "3620/3620 [==============================] - 0s 34us/sample - loss: 2.2705 - accuracy: 0.3265 - val_loss: 2.1782 - val_accuracy: 0.4051\n",
      "Epoch 3/2000\n",
      "3620/3620 [==============================] - 0s 34us/sample - loss: 2.0219 - accuracy: 0.4171 - val_loss: 1.9651 - val_accuracy: 0.5261\n",
      "Epoch 4/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 1.8027 - accuracy: 0.4950 - val_loss: 1.7924 - val_accuracy: 0.5476\n",
      "Epoch 5/2000\n",
      "3620/3620 [==============================] - 0s 44us/sample - loss: 1.6013 - accuracy: 0.5727 - val_loss: 1.6466 - val_accuracy: 0.5725\n",
      "Epoch 6/2000\n",
      "3620/3620 [==============================] - 0s 47us/sample - loss: 1.4160 - accuracy: 0.6235 - val_loss: 1.5276 - val_accuracy: 0.6081\n",
      "Epoch 7/2000\n",
      "3620/3620 [==============================] - 0s 50us/sample - loss: 1.2888 - accuracy: 0.6660 - val_loss: 1.4308 - val_accuracy: 0.6189\n",
      "Epoch 8/2000\n",
      "3620/3620 [==============================] - 0s 55us/sample - loss: 1.1721 - accuracy: 0.6997 - val_loss: 1.3730 - val_accuracy: 0.6156\n",
      "Epoch 9/2000\n",
      "3620/3620 [==============================] - 0s 45us/sample - loss: 1.0628 - accuracy: 0.7262 - val_loss: 1.3237 - val_accuracy: 0.6214\n",
      "Epoch 10/2000\n",
      "3620/3620 [==============================] - 0s 47us/sample - loss: 0.9850 - accuracy: 0.7472 - val_loss: 1.2918 - val_accuracy: 0.6313\n",
      "Epoch 11/2000\n",
      "3620/3620 [==============================] - 0s 55us/sample - loss: 0.9077 - accuracy: 0.7727 - val_loss: 1.2625 - val_accuracy: 0.6263\n",
      "Epoch 12/2000\n",
      "3620/3620 [==============================] - 0s 46us/sample - loss: 0.8456 - accuracy: 0.7961 - val_loss: 1.2351 - val_accuracy: 0.6379\n",
      "Epoch 13/2000\n",
      "3620/3620 [==============================] - 0s 44us/sample - loss: 0.8009 - accuracy: 0.7992 - val_loss: 1.2458 - val_accuracy: 0.6454\n",
      "Epoch 14/2000\n",
      "3620/3620 [==============================] - 0s 54us/sample - loss: 0.7357 - accuracy: 0.8309 - val_loss: 1.2614 - val_accuracy: 0.6578\n",
      "Epoch 15/2000\n",
      "3620/3620 [==============================] - 0s 51us/sample - loss: 0.7019 - accuracy: 0.8304 - val_loss: 1.2323 - val_accuracy: 0.6479\n",
      "Epoch 16/2000\n",
      "3620/3620 [==============================] - 0s 55us/sample - loss: 0.6691 - accuracy: 0.8406 - val_loss: 1.2457 - val_accuracy: 0.6587\n",
      "Epoch 17/2000\n",
      "3620/3620 [==============================] - 0s 42us/sample - loss: 0.6404 - accuracy: 0.8511 - val_loss: 1.2479 - val_accuracy: 0.6578\n",
      "Epoch 18/2000\n",
      "3620/3620 [==============================] - 0s 45us/sample - loss: 0.6117 - accuracy: 0.8608 - val_loss: 1.2454 - val_accuracy: 0.6562\n",
      "Epoch 19/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.5782 - accuracy: 0.8680 - val_loss: 1.2840 - val_accuracy: 0.6537\n",
      "Epoch 20/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.5728 - accuracy: 0.8641 - val_loss: 1.2878 - val_accuracy: 0.6620\n",
      "Epoch 21/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.5467 - accuracy: 0.8782 - val_loss: 1.2556 - val_accuracy: 0.6678\n",
      "Epoch 22/2000\n",
      "3620/3620 [==============================] - 0s 35us/sample - loss: 0.5255 - accuracy: 0.8796 - val_loss: 1.2707 - val_accuracy: 0.6703\n",
      "Epoch 23/2000\n",
      "3620/3620 [==============================] - 0s 34us/sample - loss: 0.5107 - accuracy: 0.8862 - val_loss: 1.2879 - val_accuracy: 0.6669\n",
      "Epoch 24/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3620/3620 [==============================] - 0s 35us/sample - loss: 0.5107 - accuracy: 0.8829 - val_loss: 1.3494 - val_accuracy: 0.6645\n",
      "Epoch 25/2000\n",
      "3620/3620 [==============================] - 0s 36us/sample - loss: 0.4831 - accuracy: 0.8898 - val_loss: 1.3192 - val_accuracy: 0.6678\n",
      "Epoch 26/2000\n",
      "3620/3620 [==============================] - 0s 36us/sample - loss: 0.4768 - accuracy: 0.8925 - val_loss: 1.3383 - val_accuracy: 0.6620\n",
      "Epoch 27/2000\n",
      "3620/3620 [==============================] - 0s 35us/sample - loss: 0.4627 - accuracy: 0.8914 - val_loss: 1.3637 - val_accuracy: 0.6636\n",
      "Epoch 28/2000\n",
      "3620/3620 [==============================] - 0s 35us/sample - loss: 0.4523 - accuracy: 0.8964 - val_loss: 1.3698 - val_accuracy: 0.6703\n",
      "Epoch 29/2000\n",
      "3620/3620 [==============================] - 0s 35us/sample - loss: 0.4400 - accuracy: 0.9022 - val_loss: 1.3842 - val_accuracy: 0.6678\n",
      "Epoch 30/2000\n",
      "3620/3620 [==============================] - 0s 36us/sample - loss: 0.4293 - accuracy: 0.9039 - val_loss: 1.4332 - val_accuracy: 0.6620\n",
      "Epoch 31/2000\n",
      "3620/3620 [==============================] - 0s 35us/sample - loss: 0.4285 - accuracy: 0.9116 - val_loss: 1.3909 - val_accuracy: 0.6636\n",
      "Epoch 32/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.4288 - accuracy: 0.9039 - val_loss: 1.4217 - val_accuracy: 0.6578\n",
      "Epoch 33/2000\n",
      "3620/3620 [==============================] - 0s 41us/sample - loss: 0.4052 - accuracy: 0.9113 - val_loss: 1.3880 - val_accuracy: 0.6628\n",
      "Epoch 34/2000\n",
      "3620/3620 [==============================] - 0s 42us/sample - loss: 0.4039 - accuracy: 0.9180 - val_loss: 1.3876 - val_accuracy: 0.6703\n",
      "Epoch 35/2000\n",
      "3620/3620 [==============================] - 0s 41us/sample - loss: 0.3945 - accuracy: 0.9160 - val_loss: 1.4243 - val_accuracy: 0.6603\n",
      "Epoch 36/2000\n",
      "3620/3620 [==============================] - 0s 36us/sample - loss: 0.4054 - accuracy: 0.9102 - val_loss: 1.4311 - val_accuracy: 0.6669\n",
      "Epoch 37/2000\n",
      "3620/3620 [==============================] - 0s 41us/sample - loss: 0.3858 - accuracy: 0.9188 - val_loss: 1.4242 - val_accuracy: 0.6611\n",
      "Epoch 38/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3884 - accuracy: 0.9155 - val_loss: 1.4561 - val_accuracy: 0.6628\n",
      "Epoch 39/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3738 - accuracy: 0.9224 - val_loss: 1.4603 - val_accuracy: 0.6603\n",
      "Epoch 40/2000\n",
      "3620/3620 [==============================] - 0s 35us/sample - loss: 0.3591 - accuracy: 0.9282 - val_loss: 1.4396 - val_accuracy: 0.6686\n",
      "Epoch 41/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.3603 - accuracy: 0.9213 - val_loss: 1.4688 - val_accuracy: 0.6645\n",
      "Epoch 42/2000\n",
      "3620/3620 [==============================] - 0s 40us/sample - loss: 0.3600 - accuracy: 0.9309 - val_loss: 1.4933 - val_accuracy: 0.6595\n",
      "Epoch 43/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3590 - accuracy: 0.9215 - val_loss: 1.4922 - val_accuracy: 0.6645\n",
      "Epoch 44/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3517 - accuracy: 0.9282 - val_loss: 1.5303 - val_accuracy: 0.6595\n",
      "Epoch 45/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3427 - accuracy: 0.9340 - val_loss: 1.5113 - val_accuracy: 0.6611\n",
      "Epoch 46/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3363 - accuracy: 0.9345 - val_loss: 1.5130 - val_accuracy: 0.6628\n",
      "Epoch 47/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3353 - accuracy: 0.9298 - val_loss: 1.5479 - val_accuracy: 0.6570\n",
      "Epoch 48/2000\n",
      "3620/3620 [==============================] - 0s 40us/sample - loss: 0.3408 - accuracy: 0.9296 - val_loss: 1.5570 - val_accuracy: 0.6553\n",
      "Epoch 49/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3359 - accuracy: 0.9340 - val_loss: 1.5512 - val_accuracy: 0.6562\n",
      "Epoch 50/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.3256 - accuracy: 0.9365 - val_loss: 1.5415 - val_accuracy: 0.6570\n",
      "Epoch 51/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.3272 - accuracy: 0.9340 - val_loss: 1.5492 - val_accuracy: 0.6636\n",
      "Epoch 52/2000\n",
      "3620/3620 [==============================] - 0s 40us/sample - loss: 0.3239 - accuracy: 0.9320 - val_loss: 1.5617 - val_accuracy: 0.6611\n",
      "Epoch 53/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.3164 - accuracy: 0.9409 - val_loss: 1.5871 - val_accuracy: 0.6595\n",
      "Epoch 54/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.3117 - accuracy: 0.9381 - val_loss: 1.6055 - val_accuracy: 0.6562\n",
      "Epoch 55/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.3115 - accuracy: 0.9387 - val_loss: 1.5839 - val_accuracy: 0.6570\n",
      "Epoch 56/2000\n",
      "3620/3620 [==============================] - 0s 36us/sample - loss: 0.3052 - accuracy: 0.9401 - val_loss: 1.6288 - val_accuracy: 0.6611\n",
      "Epoch 57/2000\n",
      "3620/3620 [==============================] - 0s 36us/sample - loss: 0.3147 - accuracy: 0.9373 - val_loss: 1.6226 - val_accuracy: 0.6645\n",
      "Epoch 58/2000\n",
      "3620/3620 [==============================] - 0s 39us/sample - loss: 0.2991 - accuracy: 0.9423 - val_loss: 1.6228 - val_accuracy: 0.6686\n",
      "Epoch 59/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.2948 - accuracy: 0.9420 - val_loss: 1.6258 - val_accuracy: 0.6645\n",
      "Epoch 60/2000\n",
      "3620/3620 [==============================] - 0s 41us/sample - loss: 0.3045 - accuracy: 0.9367 - val_loss: 1.6429 - val_accuracy: 0.6587\n",
      "Epoch 61/2000\n",
      "3620/3620 [==============================] - 0s 40us/sample - loss: 0.2912 - accuracy: 0.9478 - val_loss: 1.6424 - val_accuracy: 0.6620\n",
      "Epoch 62/2000\n",
      "3620/3620 [==============================] - 0s 39us/sample - loss: 0.2986 - accuracy: 0.9439 - val_loss: 1.6321 - val_accuracy: 0.6620\n",
      "Epoch 63/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.2953 - accuracy: 0.9436 - val_loss: 1.6575 - val_accuracy: 0.6578\n",
      "Epoch 64/2000\n",
      "3620/3620 [==============================] - 0s 37us/sample - loss: 0.2755 - accuracy: 0.9506 - val_loss: 1.6800 - val_accuracy: 0.6636\n",
      "Epoch 65/2000\n",
      "3620/3620 [==============================] - 0s 38us/sample - loss: 0.2875 - accuracy: 0.9459 - val_loss: 1.6447 - val_accuracy: 0.6678\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5430 samples, validate on 1810 samples\n",
      "Epoch 1/2000\n",
      "5430/5430 [==============================] - 1s 125us/sample - loss: 2.4402 - accuracy: 0.2416 - val_loss: 2.2546 - val_accuracy: 0.4994\n",
      "Epoch 2/2000\n",
      "5430/5430 [==============================] - 0s 40us/sample - loss: 2.0680 - accuracy: 0.3829 - val_loss: 1.9079 - val_accuracy: 0.5890\n",
      "Epoch 3/2000\n",
      "5430/5430 [==============================] - 0s 41us/sample - loss: 1.7458 - accuracy: 0.4810 - val_loss: 1.6353 - val_accuracy: 0.6348\n",
      "Epoch 4/2000\n",
      "5430/5430 [==============================] - 0s 42us/sample - loss: 1.5122 - accuracy: 0.5650 - val_loss: 1.4431 - val_accuracy: 0.6685\n",
      "Epoch 5/2000\n",
      "5430/5430 [==============================] - 0s 40us/sample - loss: 1.3139 - accuracy: 0.6300 - val_loss: 1.2920 - val_accuracy: 0.7088\n",
      "Epoch 6/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 1.1654 - accuracy: 0.6843 - val_loss: 1.1799 - val_accuracy: 0.7260\n",
      "Epoch 7/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 1.0475 - accuracy: 0.7228 - val_loss: 1.1022 - val_accuracy: 0.7188\n",
      "Epoch 8/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.9422 - accuracy: 0.7527 - val_loss: 1.0380 - val_accuracy: 0.7398\n",
      "Epoch 9/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.8668 - accuracy: 0.7663 - val_loss: 0.9921 - val_accuracy: 0.7464\n",
      "Epoch 10/2000\n",
      "5430/5430 [==============================] - 0s 40us/sample - loss: 0.8001 - accuracy: 0.7948 - val_loss: 0.9611 - val_accuracy: 0.7552\n",
      "Epoch 11/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.7512 - accuracy: 0.8074 - val_loss: 0.9427 - val_accuracy: 0.7497\n",
      "Epoch 12/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.7075 - accuracy: 0.8177 - val_loss: 0.9278 - val_accuracy: 0.7558\n",
      "Epoch 13/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.6689 - accuracy: 0.8300 - val_loss: 0.9097 - val_accuracy: 0.7597\n",
      "Epoch 14/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.6330 - accuracy: 0.8435 - val_loss: 0.9059 - val_accuracy: 0.7552\n",
      "Epoch 15/2000\n",
      "5430/5430 [==============================] - 0s 40us/sample - loss: 0.6076 - accuracy: 0.8510 - val_loss: 0.8873 - val_accuracy: 0.7619\n",
      "Epoch 16/2000\n",
      "5430/5430 [==============================] - 0s 35us/sample - loss: 0.5768 - accuracy: 0.8698 - val_loss: 0.8949 - val_accuracy: 0.7669\n",
      "Epoch 17/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.5674 - accuracy: 0.8610 - val_loss: 0.8906 - val_accuracy: 0.7591\n",
      "Epoch 18/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.5360 - accuracy: 0.8766 - val_loss: 0.8689 - val_accuracy: 0.7630\n",
      "Epoch 19/2000\n",
      "5430/5430 [==============================] - 0s 35us/sample - loss: 0.5289 - accuracy: 0.8722 - val_loss: 0.8872 - val_accuracy: 0.7657\n",
      "Epoch 20/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.5061 - accuracy: 0.8779 - val_loss: 0.8856 - val_accuracy: 0.7657\n",
      "Epoch 21/2000\n",
      "5430/5430 [==============================] - 0s 35us/sample - loss: 0.4925 - accuracy: 0.8855 - val_loss: 0.8669 - val_accuracy: 0.7729\n",
      "Epoch 22/2000\n",
      "5430/5430 [==============================] - 0s 33us/sample - loss: 0.4848 - accuracy: 0.8827 - val_loss: 0.8696 - val_accuracy: 0.7691\n",
      "Epoch 23/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.4738 - accuracy: 0.8860 - val_loss: 0.8669 - val_accuracy: 0.7746\n",
      "Epoch 24/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.4650 - accuracy: 0.8961 - val_loss: 0.8686 - val_accuracy: 0.7624\n",
      "Epoch 25/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.4553 - accuracy: 0.8956 - val_loss: 0.8895 - val_accuracy: 0.7575\n",
      "Epoch 26/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.4425 - accuracy: 0.8987 - val_loss: 0.8767 - val_accuracy: 0.7669\n",
      "Epoch 27/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.4371 - accuracy: 0.8932 - val_loss: 0.8852 - val_accuracy: 0.7680\n",
      "Epoch 28/2000\n",
      "5430/5430 [==============================] - 0s 40us/sample - loss: 0.4255 - accuracy: 0.9031 - val_loss: 0.8755 - val_accuracy: 0.7685\n",
      "Epoch 29/2000\n",
      "5430/5430 [==============================] - 0s 40us/sample - loss: 0.4199 - accuracy: 0.9052 - val_loss: 0.8923 - val_accuracy: 0.7591\n",
      "Epoch 30/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.4176 - accuracy: 0.9035 - val_loss: 0.8944 - val_accuracy: 0.7680\n",
      "Epoch 31/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.3997 - accuracy: 0.9074 - val_loss: 0.8999 - val_accuracy: 0.7696\n",
      "Epoch 32/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.3975 - accuracy: 0.9112 - val_loss: 0.8774 - val_accuracy: 0.7713\n",
      "Epoch 33/2000\n",
      "5430/5430 [==============================] - 0s 40us/sample - loss: 0.3935 - accuracy: 0.9131 - val_loss: 0.8942 - val_accuracy: 0.7669\n",
      "Epoch 34/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.3919 - accuracy: 0.9092 - val_loss: 0.9030 - val_accuracy: 0.7696\n",
      "Epoch 35/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.3821 - accuracy: 0.9182 - val_loss: 0.9055 - val_accuracy: 0.7674\n",
      "Epoch 36/2000\n",
      "5430/5430 [==============================] - 0s 40us/sample - loss: 0.3777 - accuracy: 0.9190 - val_loss: 0.8968 - val_accuracy: 0.7641\n",
      "Epoch 37/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.3655 - accuracy: 0.9179 - val_loss: 0.8903 - val_accuracy: 0.7713\n",
      "Epoch 38/2000\n",
      "5430/5430 [==============================] - 0s 35us/sample - loss: 0.3630 - accuracy: 0.9158 - val_loss: 0.8900 - val_accuracy: 0.7669\n",
      "Epoch 39/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.3495 - accuracy: 0.9256 - val_loss: 0.9232 - val_accuracy: 0.7552\n",
      "Epoch 40/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.3522 - accuracy: 0.9252 - val_loss: 0.9116 - val_accuracy: 0.7635\n",
      "Epoch 41/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.3583 - accuracy: 0.9199 - val_loss: 0.9121 - val_accuracy: 0.7696\n",
      "Epoch 42/2000\n",
      "5430/5430 [==============================] - 0s 42us/sample - loss: 0.3470 - accuracy: 0.9221 - val_loss: 0.9166 - val_accuracy: 0.7635\n",
      "Epoch 43/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.3422 - accuracy: 0.9245 - val_loss: 0.9093 - val_accuracy: 0.7773\n",
      "Epoch 44/2000\n",
      "5430/5430 [==============================] - 0s 41us/sample - loss: 0.3299 - accuracy: 0.9341 - val_loss: 0.9135 - val_accuracy: 0.7707\n",
      "Epoch 45/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.3403 - accuracy: 0.9241 - val_loss: 0.9014 - val_accuracy: 0.7663\n",
      "Epoch 46/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.3290 - accuracy: 0.9315 - val_loss: 0.9441 - val_accuracy: 0.7586\n",
      "Epoch 47/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.3336 - accuracy: 0.9284 - val_loss: 0.9375 - val_accuracy: 0.7602\n",
      "Epoch 48/2000\n",
      "5430/5430 [==============================] - 0s 41us/sample - loss: 0.3254 - accuracy: 0.9308 - val_loss: 0.9228 - val_accuracy: 0.7685\n",
      "Epoch 49/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.3223 - accuracy: 0.9300 - val_loss: 0.9139 - val_accuracy: 0.7685\n",
      "Epoch 50/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.3261 - accuracy: 0.9309 - val_loss: 0.9390 - val_accuracy: 0.7702\n",
      "Epoch 51/2000\n",
      "5430/5430 [==============================] - 0s 41us/sample - loss: 0.3293 - accuracy: 0.9289 - val_loss: 0.9221 - val_accuracy: 0.7685\n",
      "Epoch 52/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.3175 - accuracy: 0.9293 - val_loss: 0.9140 - val_accuracy: 0.7729\n",
      "Epoch 53/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.3071 - accuracy: 0.9344 - val_loss: 0.9218 - val_accuracy: 0.7663\n",
      "Epoch 54/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.3022 - accuracy: 0.9361 - val_loss: 0.9294 - val_accuracy: 0.7735\n",
      "Epoch 55/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.3115 - accuracy: 0.9366 - val_loss: 0.9216 - val_accuracy: 0.7735\n",
      "Epoch 56/2000\n",
      "5430/5430 [==============================] - 0s 35us/sample - loss: 0.3012 - accuracy: 0.9341 - val_loss: 0.9433 - val_accuracy: 0.7696\n",
      "Epoch 57/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.2936 - accuracy: 0.9392 - val_loss: 0.9321 - val_accuracy: 0.7696\n",
      "Epoch 58/2000\n",
      "5430/5430 [==============================] - 0s 41us/sample - loss: 0.2910 - accuracy: 0.9394 - val_loss: 0.9583 - val_accuracy: 0.7713\n",
      "Epoch 59/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.2984 - accuracy: 0.9363 - val_loss: 0.9567 - val_accuracy: 0.7657\n",
      "Epoch 60/2000\n",
      "5430/5430 [==============================] - 0s 35us/sample - loss: 0.2869 - accuracy: 0.9385 - val_loss: 0.9484 - val_accuracy: 0.7724\n",
      "Epoch 61/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.2914 - accuracy: 0.9392 - val_loss: 0.9578 - val_accuracy: 0.7773\n",
      "Epoch 62/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.2859 - accuracy: 0.9413 - val_loss: 0.9526 - val_accuracy: 0.7779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.2862 - accuracy: 0.9440 - val_loss: 0.9536 - val_accuracy: 0.7641\n",
      "Epoch 64/2000\n",
      "5430/5430 [==============================] - 0s 43us/sample - loss: 0.2768 - accuracy: 0.9420 - val_loss: 0.9666 - val_accuracy: 0.7669\n",
      "Epoch 65/2000\n",
      "5430/5430 [==============================] - 0s 39us/sample - loss: 0.2879 - accuracy: 0.9400 - val_loss: 0.9718 - val_accuracy: 0.7680\n",
      "Epoch 66/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.2798 - accuracy: 0.9413 - val_loss: 0.9699 - val_accuracy: 0.7663\n",
      "Epoch 67/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.2783 - accuracy: 0.9466 - val_loss: 0.9522 - val_accuracy: 0.7685\n",
      "Epoch 68/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.2744 - accuracy: 0.9427 - val_loss: 0.9713 - val_accuracy: 0.7657\n",
      "Epoch 69/2000\n",
      "5430/5430 [==============================] - 0s 38us/sample - loss: 0.2744 - accuracy: 0.9398 - val_loss: 0.9742 - val_accuracy: 0.7635\n",
      "Epoch 70/2000\n",
      "5430/5430 [==============================] - 0s 36us/sample - loss: 0.2647 - accuracy: 0.9473 - val_loss: 0.9664 - val_accuracy: 0.7652\n",
      "Epoch 71/2000\n",
      "5430/5430 [==============================] - 0s 37us/sample - loss: 0.2745 - accuracy: 0.9446 - val_loss: 0.9512 - val_accuracy: 0.7680\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7240 samples, validate on 2414 samples\n",
      "Epoch 1/2000\n",
      "7240/7240 [==============================] - 1s 116us/sample - loss: 2.4800 - accuracy: 0.1932 - val_loss: 2.2732 - val_accuracy: 0.4329\n",
      "Epoch 2/2000\n",
      "7240/7240 [==============================] - 0s 36us/sample - loss: 2.0263 - accuracy: 0.4061 - val_loss: 1.8275 - val_accuracy: 0.5853\n",
      "Epoch 3/2000\n",
      "7240/7240 [==============================] - 0s 39us/sample - loss: 1.6390 - accuracy: 0.5239 - val_loss: 1.5161 - val_accuracy: 0.6404\n",
      "Epoch 4/2000\n",
      "7240/7240 [==============================] - 0s 38us/sample - loss: 1.3723 - accuracy: 0.6012 - val_loss: 1.3040 - val_accuracy: 0.6769\n",
      "Epoch 5/2000\n",
      "7240/7240 [==============================] - 0s 37us/sample - loss: 1.1769 - accuracy: 0.6627 - val_loss: 1.1563 - val_accuracy: 0.7879\n",
      "Epoch 6/2000\n",
      "7240/7240 [==============================] - 0s 36us/sample - loss: 1.0197 - accuracy: 0.7196 - val_loss: 1.0430 - val_accuracy: 0.7709\n",
      "Epoch 7/2000\n",
      "7240/7240 [==============================] - 0s 35us/sample - loss: 0.9041 - accuracy: 0.7576 - val_loss: 0.9735 - val_accuracy: 0.7962\n",
      "Epoch 8/2000\n",
      "7240/7240 [==============================] - 0s 35us/sample - loss: 0.8194 - accuracy: 0.7711 - val_loss: 0.9150 - val_accuracy: 0.8136\n",
      "Epoch 9/2000\n",
      "7240/7240 [==============================] - 0s 37us/sample - loss: 0.7529 - accuracy: 0.8035 - val_loss: 0.8791 - val_accuracy: 0.8053\n",
      "Epoch 10/2000\n",
      "7240/7240 [==============================] - 0s 35us/sample - loss: 0.6923 - accuracy: 0.8192 - val_loss: 0.8577 - val_accuracy: 0.8070\n",
      "Epoch 11/2000\n",
      "7240/7240 [==============================] - 0s 36us/sample - loss: 0.6381 - accuracy: 0.8395 - val_loss: 0.8301 - val_accuracy: 0.8107\n",
      "Epoch 12/2000\n",
      "7240/7240 [==============================] - 0s 37us/sample - loss: 0.6148 - accuracy: 0.8409 - val_loss: 0.8034 - val_accuracy: 0.8252\n",
      "Epoch 13/2000\n",
      "7240/7240 [==============================] - 0s 35us/sample - loss: 0.5841 - accuracy: 0.8562 - val_loss: 0.7814 - val_accuracy: 0.8248\n",
      "Epoch 14/2000\n",
      "7240/7240 [==============================] - 0s 37us/sample - loss: 0.5491 - accuracy: 0.8655 - val_loss: 0.7813 - val_accuracy: 0.8157\n",
      "Epoch 15/2000\n",
      "7240/7240 [==============================] - 0s 36us/sample - loss: 0.5218 - accuracy: 0.8727 - val_loss: 0.7620 - val_accuracy: 0.8285\n",
      "Epoch 16/2000\n",
      "7240/7240 [==============================] - 0s 36us/sample - loss: 0.5014 - accuracy: 0.8782 - val_loss: 0.7477 - val_accuracy: 0.8351\n",
      "Epoch 17/2000\n",
      "7240/7240 [==============================] - 0s 36us/sample - loss: 0.4848 - accuracy: 0.8812 - val_loss: 0.7485 - val_accuracy: 0.8318\n",
      "Epoch 18/2000\n",
      "7240/7240 [==============================] - 0s 36us/sample - loss: 0.4658 - accuracy: 0.8880 - val_loss: 0.7474 - val_accuracy: 0.8268\n",
      "Epoch 19/2000\n",
      "7240/7240 [==============================] - 0s 38us/sample - loss: 0.4574 - accuracy: 0.8878 - val_loss: 0.7482 - val_accuracy: 0.8306\n",
      "Epoch 20/2000\n",
      "7240/7240 [==============================] - 0s 36us/sample - loss: 0.4415 - accuracy: 0.8967 - val_loss: 0.7403 - val_accuracy: 0.8297\n",
      "Epoch 21/2000\n",
      "7240/7240 [==============================] - 0s 35us/sample - loss: 0.4273 - accuracy: 0.8975 - val_loss: 0.7447 - val_accuracy: 0.8310\n",
      "Epoch 22/2000\n",
      "7240/7240 [==============================] - 0s 37us/sample - loss: 0.4202 - accuracy: 0.9033 - val_loss: 0.7447 - val_accuracy: 0.8264\n",
      "Epoch 23/2000\n",
      "7240/7240 [==============================] - 0s 38us/sample - loss: 0.3999 - accuracy: 0.9109 - val_loss: 0.7418 - val_accuracy: 0.8322\n",
      "Epoch 24/2000\n",
      "7240/7240 [==============================] - 0s 39us/sample - loss: 0.4080 - accuracy: 0.9058 - val_loss: 0.7400 - val_accuracy: 0.8318\n",
      "Epoch 25/2000\n",
      "7240/7240 [==============================] - 0s 29us/sample - loss: 0.3883 - accuracy: 0.9084 - val_loss: 0.7385 - val_accuracy: 0.8376\n",
      "Epoch 26/2000\n",
      "7240/7240 [==============================] - 0s 29us/sample - loss: 0.3882 - accuracy: 0.9097 - val_loss: 0.7541 - val_accuracy: 0.8380\n",
      "Epoch 27/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.3736 - accuracy: 0.9162 - val_loss: 0.7400 - val_accuracy: 0.8384\n",
      "Epoch 28/2000\n",
      "7240/7240 [==============================] - 0s 39us/sample - loss: 0.3620 - accuracy: 0.9207 - val_loss: 0.7418 - val_accuracy: 0.8297\n",
      "Epoch 29/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.3587 - accuracy: 0.9170 - val_loss: 0.7406 - val_accuracy: 0.8384\n",
      "Epoch 30/2000\n",
      "7240/7240 [==============================] - 0s 30us/sample - loss: 0.3514 - accuracy: 0.9211 - val_loss: 0.7521 - val_accuracy: 0.8339\n",
      "Epoch 31/2000\n",
      "7240/7240 [==============================] - 0s 29us/sample - loss: 0.3507 - accuracy: 0.9222 - val_loss: 0.7494 - val_accuracy: 0.8343\n",
      "Epoch 32/2000\n",
      "7240/7240 [==============================] - 0s 29us/sample - loss: 0.3474 - accuracy: 0.9207 - val_loss: 0.7515 - val_accuracy: 0.8306\n",
      "Epoch 33/2000\n",
      "7240/7240 [==============================] - 0s 37us/sample - loss: 0.3456 - accuracy: 0.9213 - val_loss: 0.7541 - val_accuracy: 0.8285\n",
      "Epoch 34/2000\n",
      "7240/7240 [==============================] - 0s 41us/sample - loss: 0.3366 - accuracy: 0.9242 - val_loss: 0.7454 - val_accuracy: 0.8351\n",
      "Epoch 35/2000\n",
      "7240/7240 [==============================] - 0s 38us/sample - loss: 0.3335 - accuracy: 0.9251 - val_loss: 0.7655 - val_accuracy: 0.8314\n",
      "Epoch 36/2000\n",
      "7240/7240 [==============================] - 0s 38us/sample - loss: 0.3287 - accuracy: 0.9276 - val_loss: 0.7447 - val_accuracy: 0.8389\n",
      "Epoch 37/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.3279 - accuracy: 0.9231 - val_loss: 0.7501 - val_accuracy: 0.8364\n",
      "Epoch 38/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.3226 - accuracy: 0.9286 - val_loss: 0.7610 - val_accuracy: 0.8318\n",
      "Epoch 39/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.3252 - accuracy: 0.9242 - val_loss: 0.7661 - val_accuracy: 0.8306\n",
      "Epoch 40/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.3171 - accuracy: 0.9280 - val_loss: 0.7570 - val_accuracy: 0.8384\n",
      "Epoch 41/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.3145 - accuracy: 0.9286 - val_loss: 0.7778 - val_accuracy: 0.8310\n",
      "Epoch 42/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.3053 - accuracy: 0.9330 - val_loss: 0.7786 - val_accuracy: 0.8306\n",
      "Epoch 43/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2992 - accuracy: 0.9355 - val_loss: 0.7679 - val_accuracy: 0.8393\n",
      "Epoch 44/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2995 - accuracy: 0.9354 - val_loss: 0.7583 - val_accuracy: 0.8368\n",
      "Epoch 45/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.3018 - accuracy: 0.9319 - val_loss: 0.7644 - val_accuracy: 0.8389\n",
      "Epoch 46/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2918 - accuracy: 0.9383 - val_loss: 0.7705 - val_accuracy: 0.8384\n",
      "Epoch 47/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2895 - accuracy: 0.9370 - val_loss: 0.7604 - val_accuracy: 0.8418\n",
      "Epoch 48/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2848 - accuracy: 0.9436 - val_loss: 0.7820 - val_accuracy: 0.8331\n",
      "Epoch 49/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2799 - accuracy: 0.9388 - val_loss: 0.7890 - val_accuracy: 0.8372\n",
      "Epoch 50/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.2884 - accuracy: 0.9365 - val_loss: 0.7910 - val_accuracy: 0.8343\n",
      "Epoch 51/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2812 - accuracy: 0.9405 - val_loss: 0.7765 - val_accuracy: 0.8384\n",
      "Epoch 52/2000\n",
      "7240/7240 [==============================] - 0s 51us/sample - loss: 0.2806 - accuracy: 0.9395 - val_loss: 0.7892 - val_accuracy: 0.8376\n",
      "Epoch 53/2000\n",
      "7240/7240 [==============================] - 0s 34us/sample - loss: 0.2746 - accuracy: 0.9391 - val_loss: 0.8014 - val_accuracy: 0.8314\n",
      "Epoch 54/2000\n",
      "7240/7240 [==============================] - 0s 47us/sample - loss: 0.2743 - accuracy: 0.9425 - val_loss: 0.7958 - val_accuracy: 0.8335\n",
      "Epoch 55/2000\n",
      "7240/7240 [==============================] - 0s 37us/sample - loss: 0.2713 - accuracy: 0.9401 - val_loss: 0.7856 - val_accuracy: 0.8380\n",
      "Epoch 56/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.2718 - accuracy: 0.9399 - val_loss: 0.7991 - val_accuracy: 0.8314\n",
      "Epoch 57/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2675 - accuracy: 0.9414 - val_loss: 0.7962 - val_accuracy: 0.8360\n",
      "Epoch 58/2000\n",
      "7240/7240 [==============================] - 0s 31us/sample - loss: 0.2629 - accuracy: 0.9439 - val_loss: 0.8088 - val_accuracy: 0.8297\n",
      "Epoch 59/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2592 - accuracy: 0.9430 - val_loss: 0.7978 - val_accuracy: 0.8331\n",
      "Epoch 60/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2561 - accuracy: 0.9457 - val_loss: 0.7998 - val_accuracy: 0.8368\n",
      "Epoch 61/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2607 - accuracy: 0.9442 - val_loss: 0.7880 - val_accuracy: 0.8380\n",
      "Epoch 62/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2611 - accuracy: 0.9432 - val_loss: 0.8255 - val_accuracy: 0.8277\n",
      "Epoch 63/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2623 - accuracy: 0.9396 - val_loss: 0.8091 - val_accuracy: 0.8360\n",
      "Epoch 64/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2617 - accuracy: 0.9414 - val_loss: 0.7964 - val_accuracy: 0.8314\n",
      "Epoch 65/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2525 - accuracy: 0.9471 - val_loss: 0.7903 - val_accuracy: 0.8364\n",
      "Epoch 66/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2531 - accuracy: 0.9460 - val_loss: 0.7921 - val_accuracy: 0.8347\n",
      "Epoch 67/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.2450 - accuracy: 0.9482 - val_loss: 0.8131 - val_accuracy: 0.8318\n",
      "Epoch 68/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2495 - accuracy: 0.9459 - val_loss: 0.8177 - val_accuracy: 0.8297\n",
      "Epoch 69/2000\n",
      "7240/7240 [==============================] - 0s 31us/sample - loss: 0.2466 - accuracy: 0.9464 - val_loss: 0.7956 - val_accuracy: 0.8384\n",
      "Epoch 70/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2402 - accuracy: 0.9478 - val_loss: 0.8119 - val_accuracy: 0.8306\n",
      "Epoch 71/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2450 - accuracy: 0.9461 - val_loss: 0.7937 - val_accuracy: 0.8380\n",
      "Epoch 72/2000\n",
      "7240/7240 [==============================] - 0s 31us/sample - loss: 0.2436 - accuracy: 0.9459 - val_loss: 0.8039 - val_accuracy: 0.8376\n",
      "Epoch 73/2000\n",
      "7240/7240 [==============================] - 0s 33us/sample - loss: 0.2437 - accuracy: 0.9457 - val_loss: 0.7999 - val_accuracy: 0.8360\n",
      "Epoch 74/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2427 - accuracy: 0.9503 - val_loss: 0.8256 - val_accuracy: 0.8339\n",
      "Epoch 75/2000\n",
      "7240/7240 [==============================] - 0s 32us/sample - loss: 0.2323 - accuracy: 0.9486 - val_loss: 0.8097 - val_accuracy: 0.8335\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9051 samples, validate on 3017 samples\n",
      "Epoch 1/2000\n",
      "9051/9051 [==============================] - 1s 67us/sample - loss: 2.3259 - accuracy: 0.2667 - val_loss: 2.0081 - val_accuracy: 0.5545\n",
      "Epoch 2/2000\n",
      "9051/9051 [==============================] - 0s 51us/sample - loss: 1.7578 - accuracy: 0.4917 - val_loss: 1.5412 - val_accuracy: 0.6845\n",
      "Epoch 3/2000\n",
      "9051/9051 [==============================] - 0s 42us/sample - loss: 1.3712 - accuracy: 0.6167 - val_loss: 1.2513 - val_accuracy: 0.7640\n",
      "Epoch 4/2000\n",
      "9051/9051 [==============================] - 0s 33us/sample - loss: 1.1184 - accuracy: 0.7031 - val_loss: 1.0499 - val_accuracy: 0.7938\n",
      "Epoch 5/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.9438 - accuracy: 0.7548 - val_loss: 0.9303 - val_accuracy: 0.7793\n",
      "Epoch 6/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.8160 - accuracy: 0.7956 - val_loss: 0.8549 - val_accuracy: 0.7822\n",
      "Epoch 7/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.7139 - accuracy: 0.8233 - val_loss: 0.8061 - val_accuracy: 0.7842\n",
      "Epoch 8/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.6546 - accuracy: 0.8392 - val_loss: 0.7741 - val_accuracy: 0.7899\n",
      "Epoch 9/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.6020 - accuracy: 0.8561 - val_loss: 0.7480 - val_accuracy: 0.7945\n",
      "Epoch 10/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.5583 - accuracy: 0.8636 - val_loss: 0.7389 - val_accuracy: 0.7909\n",
      "Epoch 11/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.5182 - accuracy: 0.8808 - val_loss: 0.7342 - val_accuracy: 0.7918\n",
      "Epoch 12/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.5000 - accuracy: 0.8833 - val_loss: 0.7144 - val_accuracy: 0.8028\n",
      "Epoch 13/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.4727 - accuracy: 0.8902 - val_loss: 0.7181 - val_accuracy: 0.7978\n",
      "Epoch 14/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.4509 - accuracy: 0.8980 - val_loss: 0.7233 - val_accuracy: 0.7981\n",
      "Epoch 15/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.4319 - accuracy: 0.9006 - val_loss: 0.7249 - val_accuracy: 0.8064\n",
      "Epoch 16/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.4124 - accuracy: 0.9068 - val_loss: 0.7391 - val_accuracy: 0.7955\n",
      "Epoch 17/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.4065 - accuracy: 0.9073 - val_loss: 0.7301 - val_accuracy: 0.7998\n",
      "Epoch 18/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3957 - accuracy: 0.9124 - val_loss: 0.7302 - val_accuracy: 0.7971\n",
      "Epoch 19/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3779 - accuracy: 0.9192 - val_loss: 0.7338 - val_accuracy: 0.7991\n",
      "Epoch 20/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3823 - accuracy: 0.9113 - val_loss: 0.7188 - val_accuracy: 0.8011\n",
      "Epoch 21/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.3671 - accuracy: 0.9224 - val_loss: 0.7388 - val_accuracy: 0.8008\n",
      "Epoch 22/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3658 - accuracy: 0.9208 - val_loss: 0.7480 - val_accuracy: 0.8001\n",
      "Epoch 23/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3508 - accuracy: 0.9241 - val_loss: 0.7491 - val_accuracy: 0.8074\n",
      "Epoch 24/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3408 - accuracy: 0.9264 - val_loss: 0.7502 - val_accuracy: 0.8005\n",
      "Epoch 25/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.3385 - accuracy: 0.9295 - val_loss: 0.7423 - val_accuracy: 0.8025\n",
      "Epoch 26/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3286 - accuracy: 0.9307 - val_loss: 0.7479 - val_accuracy: 0.8034\n",
      "Epoch 27/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3281 - accuracy: 0.9313 - val_loss: 0.7487 - val_accuracy: 0.8054\n",
      "Epoch 28/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3150 - accuracy: 0.9342 - val_loss: 0.7571 - val_accuracy: 0.8025\n",
      "Epoch 29/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3137 - accuracy: 0.9349 - val_loss: 0.7781 - val_accuracy: 0.8044\n",
      "Epoch 30/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3091 - accuracy: 0.9363 - val_loss: 0.7661 - val_accuracy: 0.8091\n",
      "Epoch 31/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.3050 - accuracy: 0.9349 - val_loss: 0.7694 - val_accuracy: 0.8001\n",
      "Epoch 32/2000\n",
      "9051/9051 [==============================] - 0s 33us/sample - loss: 0.3052 - accuracy: 0.9366 - val_loss: 0.7623 - val_accuracy: 0.8074\n",
      "Epoch 33/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2973 - accuracy: 0.9354 - val_loss: 0.7716 - val_accuracy: 0.8048\n",
      "Epoch 34/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2906 - accuracy: 0.9406 - val_loss: 0.7729 - val_accuracy: 0.7995\n",
      "Epoch 35/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2931 - accuracy: 0.9410 - val_loss: 0.7678 - val_accuracy: 0.8054\n",
      "Epoch 36/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2879 - accuracy: 0.9422 - val_loss: 0.7690 - val_accuracy: 0.8088\n",
      "Epoch 37/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2795 - accuracy: 0.9416 - val_loss: 0.7776 - val_accuracy: 0.8068\n",
      "Epoch 38/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2751 - accuracy: 0.9427 - val_loss: 0.7853 - val_accuracy: 0.8025\n",
      "Epoch 39/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2725 - accuracy: 0.9453 - val_loss: 0.8043 - val_accuracy: 0.8001\n",
      "Epoch 40/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2719 - accuracy: 0.9449 - val_loss: 0.7962 - val_accuracy: 0.7991\n",
      "Epoch 41/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2690 - accuracy: 0.9454 - val_loss: 0.8367 - val_accuracy: 0.7915\n",
      "Epoch 42/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2628 - accuracy: 0.9440 - val_loss: 0.7899 - val_accuracy: 0.8025\n",
      "Epoch 43/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2604 - accuracy: 0.9470 - val_loss: 0.7629 - val_accuracy: 0.8104\n",
      "Epoch 44/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2572 - accuracy: 0.9493 - val_loss: 0.8153 - val_accuracy: 0.7998\n",
      "Epoch 45/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2550 - accuracy: 0.9494 - val_loss: 0.8258 - val_accuracy: 0.7899\n",
      "Epoch 46/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2558 - accuracy: 0.9483 - val_loss: 0.8110 - val_accuracy: 0.8015\n",
      "Epoch 47/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2504 - accuracy: 0.9493 - val_loss: 0.7864 - val_accuracy: 0.8074\n",
      "Epoch 48/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2534 - accuracy: 0.9465 - val_loss: 0.7894 - val_accuracy: 0.8058\n",
      "Epoch 49/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2529 - accuracy: 0.9444 - val_loss: 0.8073 - val_accuracy: 0.8001\n",
      "Epoch 50/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2461 - accuracy: 0.9504 - val_loss: 0.8132 - val_accuracy: 0.8021\n",
      "Epoch 51/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2439 - accuracy: 0.9503 - val_loss: 0.8163 - val_accuracy: 0.8061\n",
      "Epoch 52/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2387 - accuracy: 0.9517 - val_loss: 0.8163 - val_accuracy: 0.7978\n",
      "Epoch 53/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2420 - accuracy: 0.9517 - val_loss: 0.8101 - val_accuracy: 0.8038\n",
      "Epoch 54/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2432 - accuracy: 0.9491 - val_loss: 0.7854 - val_accuracy: 0.8091\n",
      "Epoch 55/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2309 - accuracy: 0.9547 - val_loss: 0.8250 - val_accuracy: 0.8031\n",
      "Epoch 56/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2349 - accuracy: 0.9512 - val_loss: 0.8331 - val_accuracy: 0.7995\n",
      "Epoch 57/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2320 - accuracy: 0.9513 - val_loss: 0.8480 - val_accuracy: 0.7995\n",
      "Epoch 58/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2294 - accuracy: 0.9544 - val_loss: 0.8214 - val_accuracy: 0.8021\n",
      "Epoch 59/2000\n",
      "9051/9051 [==============================] - 0s 32us/sample - loss: 0.2324 - accuracy: 0.9519 - val_loss: 0.8177 - val_accuracy: 0.8028\n",
      "Epoch 60/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2257 - accuracy: 0.9544 - val_loss: 0.8294 - val_accuracy: 0.8015\n",
      "Epoch 61/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2225 - accuracy: 0.9550 - val_loss: 0.8179 - val_accuracy: 0.7988\n",
      "Epoch 62/2000\n",
      "9051/9051 [==============================] - 0s 31us/sample - loss: 0.2203 - accuracy: 0.9550 - val_loss: 0.8181 - val_accuracy: 0.8068\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10861 samples, validate on 3621 samples\n",
      "Epoch 1/2000\n",
      "10861/10861 [==============================] - 1s 68us/sample - loss: 2.2539 - accuracy: 0.2805 - val_loss: 1.8433 - val_accuracy: 0.6457\n",
      "Epoch 2/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10861/10861 [==============================] - 0s 31us/sample - loss: 1.6208 - accuracy: 0.5086 - val_loss: 1.3593 - val_accuracy: 0.7393\n",
      "Epoch 3/2000\n",
      "10861/10861 [==============================] - 0s 31us/sample - loss: 1.2699 - accuracy: 0.6221 - val_loss: 1.0841 - val_accuracy: 0.7799\n",
      "Epoch 4/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 1.0424 - accuracy: 0.7016 - val_loss: 0.9027 - val_accuracy: 0.8219\n",
      "Epoch 5/2000\n",
      "10861/10861 [==============================] - 0s 31us/sample - loss: 0.8810 - accuracy: 0.7545 - val_loss: 0.7887 - val_accuracy: 0.8246\n",
      "Epoch 6/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.7752 - accuracy: 0.7945 - val_loss: 0.7147 - val_accuracy: 0.8246\n",
      "Epoch 7/2000\n",
      "10861/10861 [==============================] - 0s 31us/sample - loss: 0.6838 - accuracy: 0.8238 - val_loss: 0.6642 - val_accuracy: 0.8285\n",
      "Epoch 8/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.6292 - accuracy: 0.8376 - val_loss: 0.6364 - val_accuracy: 0.8288\n",
      "Epoch 9/2000\n",
      "10861/10861 [==============================] - 0s 31us/sample - loss: 0.5822 - accuracy: 0.8545 - val_loss: 0.6031 - val_accuracy: 0.8426\n",
      "Epoch 10/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.5426 - accuracy: 0.8599 - val_loss: 0.5866 - val_accuracy: 0.8418\n",
      "Epoch 11/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.5228 - accuracy: 0.8697 - val_loss: 0.5884 - val_accuracy: 0.8393\n",
      "Epoch 12/2000\n",
      "10861/10861 [==============================] - 0s 31us/sample - loss: 0.4933 - accuracy: 0.8759 - val_loss: 0.5826 - val_accuracy: 0.8360\n",
      "Epoch 13/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.4628 - accuracy: 0.8887 - val_loss: 0.5687 - val_accuracy: 0.8434\n",
      "Epoch 14/2000\n",
      "10861/10861 [==============================] - 0s 31us/sample - loss: 0.4492 - accuracy: 0.8919 - val_loss: 0.5552 - val_accuracy: 0.8431\n",
      "Epoch 15/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.4270 - accuracy: 0.8989 - val_loss: 0.5639 - val_accuracy: 0.8390\n",
      "Epoch 16/2000\n",
      "10861/10861 [==============================] - 0s 31us/sample - loss: 0.4142 - accuracy: 0.9017 - val_loss: 0.5751 - val_accuracy: 0.8390\n",
      "Epoch 17/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.4053 - accuracy: 0.9068 - val_loss: 0.5686 - val_accuracy: 0.8393\n",
      "Epoch 18/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.3939 - accuracy: 0.9120 - val_loss: 0.5565 - val_accuracy: 0.8465\n",
      "Epoch 19/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.3854 - accuracy: 0.9109 - val_loss: 0.5413 - val_accuracy: 0.8465\n",
      "Epoch 20/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.3688 - accuracy: 0.9159 - val_loss: 0.5651 - val_accuracy: 0.8431\n",
      "Epoch 21/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.3592 - accuracy: 0.9187 - val_loss: 0.5636 - val_accuracy: 0.8426\n",
      "Epoch 22/2000\n",
      "10861/10861 [==============================] - 0s 33us/sample - loss: 0.3565 - accuracy: 0.9206 - val_loss: 0.5625 - val_accuracy: 0.8442\n",
      "Epoch 23/2000\n",
      "10861/10861 [==============================] - 0s 39us/sample - loss: 0.3502 - accuracy: 0.9203 - val_loss: 0.5805 - val_accuracy: 0.8429\n",
      "Epoch 24/2000\n",
      "10861/10861 [==============================] - 0s 33us/sample - loss: 0.3485 - accuracy: 0.9194 - val_loss: 0.5640 - val_accuracy: 0.8470\n",
      "Epoch 25/2000\n",
      "10861/10861 [==============================] - 0s 36us/sample - loss: 0.3316 - accuracy: 0.9290 - val_loss: 0.5691 - val_accuracy: 0.8484\n",
      "Epoch 26/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.3300 - accuracy: 0.9272 - val_loss: 0.5616 - val_accuracy: 0.8506\n",
      "Epoch 27/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.3253 - accuracy: 0.9268 - val_loss: 0.5641 - val_accuracy: 0.8489\n",
      "Epoch 28/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.3280 - accuracy: 0.9250 - val_loss: 0.5647 - val_accuracy: 0.8495\n",
      "Epoch 29/2000\n",
      "10861/10861 [==============================] - 0s 28us/sample - loss: 0.3155 - accuracy: 0.9269 - val_loss: 0.5513 - val_accuracy: 0.8558\n",
      "Epoch 30/2000\n",
      "10861/10861 [==============================] - 0s 28us/sample - loss: 0.3138 - accuracy: 0.9310 - val_loss: 0.5443 - val_accuracy: 0.8556\n",
      "Epoch 31/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.3062 - accuracy: 0.9300 - val_loss: 0.5670 - val_accuracy: 0.8545\n",
      "Epoch 32/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.3066 - accuracy: 0.9314 - val_loss: 0.5777 - val_accuracy: 0.8442\n",
      "Epoch 33/2000\n",
      "10861/10861 [==============================] - 0s 28us/sample - loss: 0.3010 - accuracy: 0.9340 - val_loss: 0.5750 - val_accuracy: 0.8509\n",
      "Epoch 34/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2921 - accuracy: 0.9362 - val_loss: 0.5677 - val_accuracy: 0.8523\n",
      "Epoch 35/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2918 - accuracy: 0.9371 - val_loss: 0.5585 - val_accuracy: 0.8556\n",
      "Epoch 36/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2907 - accuracy: 0.9361 - val_loss: 0.5855 - val_accuracy: 0.8506\n",
      "Epoch 37/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2811 - accuracy: 0.9393 - val_loss: 0.5579 - val_accuracy: 0.8553\n",
      "Epoch 38/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2849 - accuracy: 0.9358 - val_loss: 0.5861 - val_accuracy: 0.8514\n",
      "Epoch 39/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2798 - accuracy: 0.9355 - val_loss: 0.5798 - val_accuracy: 0.8517\n",
      "Epoch 40/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2777 - accuracy: 0.9381 - val_loss: 0.5839 - val_accuracy: 0.8547\n",
      "Epoch 41/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2727 - accuracy: 0.9393 - val_loss: 0.5923 - val_accuracy: 0.8536\n",
      "Epoch 42/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2739 - accuracy: 0.9413 - val_loss: 0.5820 - val_accuracy: 0.8572\n",
      "Epoch 43/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2689 - accuracy: 0.9393 - val_loss: 0.5932 - val_accuracy: 0.8539\n",
      "Epoch 44/2000\n",
      "10861/10861 [==============================] - 0s 34us/sample - loss: 0.2668 - accuracy: 0.9391 - val_loss: 0.5744 - val_accuracy: 0.8553\n",
      "Epoch 45/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.2661 - accuracy: 0.9386 - val_loss: 0.5857 - val_accuracy: 0.8556\n",
      "Epoch 46/2000\n",
      "10861/10861 [==============================] - 0s 30us/sample - loss: 0.2601 - accuracy: 0.9442 - val_loss: 0.5969 - val_accuracy: 0.8592\n",
      "Epoch 47/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.2620 - accuracy: 0.9380 - val_loss: 0.5605 - val_accuracy: 0.8581\n",
      "Epoch 48/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2525 - accuracy: 0.9454 - val_loss: 0.5754 - val_accuracy: 0.8633\n",
      "Epoch 49/2000\n",
      "10861/10861 [==============================] - 0s 28us/sample - loss: 0.2545 - accuracy: 0.9436 - val_loss: 0.5825 - val_accuracy: 0.8600\n",
      "Epoch 50/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2536 - accuracy: 0.9437 - val_loss: 0.5892 - val_accuracy: 0.8622\n",
      "Epoch 51/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2483 - accuracy: 0.9445 - val_loss: 0.5871 - val_accuracy: 0.8603\n",
      "Epoch 52/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2471 - accuracy: 0.9445 - val_loss: 0.5928 - val_accuracy: 0.8603\n",
      "Epoch 53/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2485 - accuracy: 0.9443 - val_loss: 0.5783 - val_accuracy: 0.8616\n",
      "Epoch 54/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2414 - accuracy: 0.9462 - val_loss: 0.5807 - val_accuracy: 0.8608\n",
      "Epoch 55/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2413 - accuracy: 0.9487 - val_loss: 0.5827 - val_accuracy: 0.8625\n",
      "Epoch 56/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2344 - accuracy: 0.9488 - val_loss: 0.5900 - val_accuracy: 0.8633\n",
      "Epoch 57/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2391 - accuracy: 0.9468 - val_loss: 0.6126 - val_accuracy: 0.8622\n",
      "Epoch 58/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2373 - accuracy: 0.9465 - val_loss: 0.5715 - val_accuracy: 0.8630\n",
      "Epoch 59/2000\n",
      "10861/10861 [==============================] - 0s 32us/sample - loss: 0.2299 - accuracy: 0.9487 - val_loss: 0.5933 - val_accuracy: 0.8567\n",
      "Epoch 60/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2295 - accuracy: 0.9517 - val_loss: 0.5621 - val_accuracy: 0.8627\n",
      "Epoch 61/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2260 - accuracy: 0.9504 - val_loss: 0.5976 - val_accuracy: 0.8630\n",
      "Epoch 62/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2309 - accuracy: 0.9466 - val_loss: 0.5741 - val_accuracy: 0.8650\n",
      "Epoch 63/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2276 - accuracy: 0.9489 - val_loss: 0.5784 - val_accuracy: 0.8616\n",
      "Epoch 64/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2282 - accuracy: 0.9504 - val_loss: 0.5850 - val_accuracy: 0.8694\n",
      "Epoch 65/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2242 - accuracy: 0.9485 - val_loss: 0.5913 - val_accuracy: 0.8638\n",
      "Epoch 66/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2200 - accuracy: 0.9507 - val_loss: 0.5747 - val_accuracy: 0.8663\n",
      "Epoch 67/2000\n",
      "10861/10861 [==============================] - 0s 28us/sample - loss: 0.2190 - accuracy: 0.9514 - val_loss: 0.6054 - val_accuracy: 0.8663\n",
      "Epoch 68/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2176 - accuracy: 0.9521 - val_loss: 0.5752 - val_accuracy: 0.8683\n",
      "Epoch 69/2000\n",
      "10861/10861 [==============================] - 0s 29us/sample - loss: 0.2163 - accuracy: 0.9517 - val_loss: 0.5842 - val_accuracy: 0.8611\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12672 samples, validate on 4224 samples\n",
      "Epoch 1/2000\n",
      "12672/12672 [==============================] - 1s 51us/sample - loss: 2.2875 - accuracy: 0.3037 - val_loss: 1.8472 - val_accuracy: 0.6648\n",
      "Epoch 2/2000\n",
      "12672/12672 [==============================] - 0s 27us/sample - loss: 1.5793 - accuracy: 0.5429 - val_loss: 1.2982 - val_accuracy: 0.7438\n",
      "Epoch 3/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 1.1663 - accuracy: 0.6712 - val_loss: 1.0122 - val_accuracy: 0.8097\n",
      "Epoch 4/2000\n",
      "12672/12672 [==============================] - 0s 35us/sample - loss: 0.9223 - accuracy: 0.7530 - val_loss: 0.8761 - val_accuracy: 0.8042\n",
      "Epoch 5/2000\n",
      "12672/12672 [==============================] - 0s 35us/sample - loss: 0.7786 - accuracy: 0.7931 - val_loss: 0.7873 - val_accuracy: 0.8177\n",
      "Epoch 6/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.6736 - accuracy: 0.8240 - val_loss: 0.7431 - val_accuracy: 0.8139\n",
      "Epoch 7/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.5999 - accuracy: 0.8505 - val_loss: 0.7093 - val_accuracy: 0.8288\n",
      "Epoch 8/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.5515 - accuracy: 0.8595 - val_loss: 0.7160 - val_accuracy: 0.8348\n",
      "Epoch 9/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.5111 - accuracy: 0.8713 - val_loss: 0.6766 - val_accuracy: 0.8409\n",
      "Epoch 10/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.4815 - accuracy: 0.8821 - val_loss: 0.6721 - val_accuracy: 0.8478\n",
      "Epoch 11/2000\n",
      "12672/12672 [==============================] - 0s 29us/sample - loss: 0.4556 - accuracy: 0.8900 - val_loss: 0.6785 - val_accuracy: 0.8435\n",
      "Epoch 12/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.4399 - accuracy: 0.8913 - val_loss: 0.6749 - val_accuracy: 0.8558\n",
      "Epoch 13/2000\n",
      "12672/12672 [==============================] - 0s 32us/sample - loss: 0.4162 - accuracy: 0.9006 - val_loss: 0.6678 - val_accuracy: 0.8530\n",
      "Epoch 14/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.4018 - accuracy: 0.9065 - val_loss: 0.6647 - val_accuracy: 0.8582\n",
      "Epoch 15/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.3927 - accuracy: 0.9059 - val_loss: 0.6549 - val_accuracy: 0.8629\n",
      "Epoch 16/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.3822 - accuracy: 0.9111 - val_loss: 0.6693 - val_accuracy: 0.8561\n",
      "Epoch 17/2000\n",
      "12672/12672 [==============================] - 0s 31us/sample - loss: 0.3640 - accuracy: 0.9177 - val_loss: 0.6694 - val_accuracy: 0.8556\n",
      "Epoch 18/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.3589 - accuracy: 0.9175 - val_loss: 0.6519 - val_accuracy: 0.8688\n",
      "Epoch 19/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.3525 - accuracy: 0.9217 - val_loss: 0.6765 - val_accuracy: 0.8532\n",
      "Epoch 20/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.3407 - accuracy: 0.9241 - val_loss: 0.6553 - val_accuracy: 0.8610\n",
      "Epoch 21/2000\n",
      "12672/12672 [==============================] - 0s 31us/sample - loss: 0.3332 - accuracy: 0.9245 - val_loss: 0.6613 - val_accuracy: 0.8589\n",
      "Epoch 22/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.3318 - accuracy: 0.9229 - val_loss: 0.6560 - val_accuracy: 0.8594\n",
      "Epoch 23/2000\n",
      "12672/12672 [==============================] - 1s 42us/sample - loss: 0.3231 - accuracy: 0.9254 - val_loss: 0.6679 - val_accuracy: 0.8613\n",
      "Epoch 24/2000\n",
      "12672/12672 [==============================] - 1s 41us/sample - loss: 0.3166 - accuracy: 0.9310 - val_loss: 0.6885 - val_accuracy: 0.8580\n",
      "Epoch 25/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.3113 - accuracy: 0.9308 - val_loss: 0.6391 - val_accuracy: 0.8665\n",
      "Epoch 26/2000\n",
      "12672/12672 [==============================] - 0s 33us/sample - loss: 0.3049 - accuracy: 0.9332 - val_loss: 0.6636 - val_accuracy: 0.8665\n",
      "Epoch 27/2000\n",
      "12672/12672 [==============================] - 0s 34us/sample - loss: 0.3001 - accuracy: 0.9297 - val_loss: 0.6726 - val_accuracy: 0.8634\n",
      "Epoch 28/2000\n",
      "12672/12672 [==============================] - 1s 41us/sample - loss: 0.2908 - accuracy: 0.9370 - val_loss: 0.6318 - val_accuracy: 0.8658\n",
      "Epoch 29/2000\n",
      "12672/12672 [==============================] - 0s 33us/sample - loss: 0.2921 - accuracy: 0.9335 - val_loss: 0.6751 - val_accuracy: 0.8660\n",
      "Epoch 30/2000\n",
      "12672/12672 [==============================] - 1s 40us/sample - loss: 0.2859 - accuracy: 0.9366 - val_loss: 0.6722 - val_accuracy: 0.8598\n",
      "Epoch 31/2000\n",
      "12672/12672 [==============================] - 0s 37us/sample - loss: 0.2874 - accuracy: 0.9341 - val_loss: 0.6484 - val_accuracy: 0.8641\n",
      "Epoch 32/2000\n",
      "12672/12672 [==============================] - 0s 29us/sample - loss: 0.2848 - accuracy: 0.9361 - val_loss: 0.6702 - val_accuracy: 0.8615\n",
      "Epoch 33/2000\n",
      "12672/12672 [==============================] - 0s 31us/sample - loss: 0.2731 - accuracy: 0.9400 - val_loss: 0.6595 - val_accuracy: 0.8684\n",
      "Epoch 34/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2749 - accuracy: 0.9378 - val_loss: 0.6453 - val_accuracy: 0.8629\n",
      "Epoch 35/2000\n",
      "12672/12672 [==============================] - 0s 32us/sample - loss: 0.2656 - accuracy: 0.9418 - val_loss: 0.6556 - val_accuracy: 0.8719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2669 - accuracy: 0.9398 - val_loss: 0.6906 - val_accuracy: 0.8601\n",
      "Epoch 37/2000\n",
      "12672/12672 [==============================] - 0s 29us/sample - loss: 0.2653 - accuracy: 0.9412 - val_loss: 0.6545 - val_accuracy: 0.8670\n",
      "Epoch 38/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2616 - accuracy: 0.9406 - val_loss: 0.6689 - val_accuracy: 0.8653\n",
      "Epoch 39/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2548 - accuracy: 0.9428 - val_loss: 0.6611 - val_accuracy: 0.8686\n",
      "Epoch 40/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2500 - accuracy: 0.9445 - val_loss: 0.6240 - val_accuracy: 0.8731\n",
      "Epoch 41/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2524 - accuracy: 0.9455 - val_loss: 0.6475 - val_accuracy: 0.8717\n",
      "Epoch 42/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2486 - accuracy: 0.9450 - val_loss: 0.6318 - val_accuracy: 0.8771\n",
      "Epoch 43/2000\n",
      "12672/12672 [==============================] - 0s 29us/sample - loss: 0.2408 - accuracy: 0.9475 - val_loss: 0.6442 - val_accuracy: 0.8722\n",
      "Epoch 44/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2422 - accuracy: 0.9466 - val_loss: 0.6462 - val_accuracy: 0.8726\n",
      "Epoch 45/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2382 - accuracy: 0.9442 - val_loss: 0.6310 - val_accuracy: 0.8722\n",
      "Epoch 46/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2354 - accuracy: 0.9478 - val_loss: 0.6576 - val_accuracy: 0.8613\n",
      "Epoch 47/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2364 - accuracy: 0.9463 - val_loss: 0.6439 - val_accuracy: 0.8684\n",
      "Epoch 48/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2330 - accuracy: 0.9487 - val_loss: 0.6776 - val_accuracy: 0.8580\n",
      "Epoch 49/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2306 - accuracy: 0.9488 - val_loss: 0.6230 - val_accuracy: 0.8783\n",
      "Epoch 50/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2330 - accuracy: 0.9498 - val_loss: 0.6516 - val_accuracy: 0.8688\n",
      "Epoch 51/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2252 - accuracy: 0.9497 - val_loss: 0.6737 - val_accuracy: 0.8651\n",
      "Epoch 52/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2218 - accuracy: 0.9509 - val_loss: 0.6219 - val_accuracy: 0.8733\n",
      "Epoch 53/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2260 - accuracy: 0.9478 - val_loss: 0.6254 - val_accuracy: 0.8726\n",
      "Epoch 54/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2174 - accuracy: 0.9546 - val_loss: 0.6279 - val_accuracy: 0.8764\n",
      "Epoch 55/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2154 - accuracy: 0.9535 - val_loss: 0.6427 - val_accuracy: 0.8752\n",
      "Epoch 56/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2181 - accuracy: 0.9491 - val_loss: 0.6664 - val_accuracy: 0.8620\n",
      "Epoch 57/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2127 - accuracy: 0.9508 - val_loss: 0.6843 - val_accuracy: 0.8625\n",
      "Epoch 58/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2158 - accuracy: 0.9516 - val_loss: 0.6813 - val_accuracy: 0.8629\n",
      "Epoch 59/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2099 - accuracy: 0.9549 - val_loss: 0.6414 - val_accuracy: 0.8674\n",
      "Epoch 60/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2111 - accuracy: 0.9521 - val_loss: 0.6716 - val_accuracy: 0.8667\n",
      "Epoch 61/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2060 - accuracy: 0.9557 - val_loss: 0.6249 - val_accuracy: 0.8691\n",
      "Epoch 62/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2071 - accuracy: 0.9527 - val_loss: 0.6365 - val_accuracy: 0.8714\n",
      "Epoch 63/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.2074 - accuracy: 0.9529 - val_loss: 0.6904 - val_accuracy: 0.8688\n",
      "Epoch 64/2000\n",
      "12672/12672 [==============================] - 0s 35us/sample - loss: 0.2073 - accuracy: 0.9542 - val_loss: 0.6746 - val_accuracy: 0.8684\n",
      "Epoch 65/2000\n",
      "12672/12672 [==============================] - 0s 38us/sample - loss: 0.1969 - accuracy: 0.9572 - val_loss: 0.6221 - val_accuracy: 0.8724\n",
      "Epoch 66/2000\n",
      "12672/12672 [==============================] - 0s 36us/sample - loss: 0.2049 - accuracy: 0.9535 - val_loss: 0.6738 - val_accuracy: 0.8587\n",
      "Epoch 67/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1990 - accuracy: 0.9568 - val_loss: 0.6339 - val_accuracy: 0.8703\n",
      "Epoch 68/2000\n",
      "12672/12672 [==============================] - 0s 31us/sample - loss: 0.1985 - accuracy: 0.9558 - val_loss: 0.6412 - val_accuracy: 0.8627\n",
      "Epoch 69/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1999 - accuracy: 0.9558 - val_loss: 0.6637 - val_accuracy: 0.8646\n",
      "Epoch 70/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1953 - accuracy: 0.9564 - val_loss: 0.6365 - val_accuracy: 0.8714\n",
      "Epoch 71/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1966 - accuracy: 0.9553 - val_loss: 0.6447 - val_accuracy: 0.8705\n",
      "Epoch 72/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1906 - accuracy: 0.9560 - val_loss: 0.6766 - val_accuracy: 0.8589\n",
      "Epoch 73/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1898 - accuracy: 0.9561 - val_loss: 0.6070 - val_accuracy: 0.8795\n",
      "Epoch 74/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1894 - accuracy: 0.9576 - val_loss: 0.6324 - val_accuracy: 0.8712\n",
      "Epoch 75/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1913 - accuracy: 0.9559 - val_loss: 0.6352 - val_accuracy: 0.8759\n",
      "Epoch 76/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1869 - accuracy: 0.9576 - val_loss: 0.6705 - val_accuracy: 0.8679\n",
      "Epoch 77/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1875 - accuracy: 0.9569 - val_loss: 0.6672 - val_accuracy: 0.8648\n",
      "Epoch 78/2000\n",
      "12672/12672 [==============================] - 0s 29us/sample - loss: 0.1873 - accuracy: 0.9575 - val_loss: 0.6457 - val_accuracy: 0.8674\n",
      "Epoch 79/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1875 - accuracy: 0.9579 - val_loss: 0.6218 - val_accuracy: 0.8750\n",
      "Epoch 80/2000\n",
      "12672/12672 [==============================] - 0s 29us/sample - loss: 0.1831 - accuracy: 0.9568 - val_loss: 0.6615 - val_accuracy: 0.8575\n",
      "Epoch 81/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1872 - accuracy: 0.9580 - val_loss: 0.6389 - val_accuracy: 0.8672\n",
      "Epoch 82/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1827 - accuracy: 0.9590 - val_loss: 0.6636 - val_accuracy: 0.8672\n",
      "Epoch 83/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1804 - accuracy: 0.9605 - val_loss: 0.6071 - val_accuracy: 0.8750\n",
      "Epoch 84/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1836 - accuracy: 0.9581 - val_loss: 0.6160 - val_accuracy: 0.8714\n",
      "Epoch 85/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1768 - accuracy: 0.9587 - val_loss: 0.6398 - val_accuracy: 0.8788\n",
      "Epoch 86/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1786 - accuracy: 0.9594 - val_loss: 0.6473 - val_accuracy: 0.8767\n",
      "Epoch 87/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1795 - accuracy: 0.9602 - val_loss: 0.6285 - val_accuracy: 0.8700\n",
      "Epoch 88/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1821 - accuracy: 0.9587 - val_loss: 0.6508 - val_accuracy: 0.8584\n",
      "Epoch 89/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1772 - accuracy: 0.9613 - val_loss: 0.6372 - val_accuracy: 0.8662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1752 - accuracy: 0.9601 - val_loss: 0.6552 - val_accuracy: 0.8603\n",
      "Epoch 91/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1775 - accuracy: 0.9606 - val_loss: 0.6339 - val_accuracy: 0.8745\n",
      "Epoch 92/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1748 - accuracy: 0.9611 - val_loss: 0.6851 - val_accuracy: 0.8646\n",
      "Epoch 93/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1724 - accuracy: 0.9613 - val_loss: 0.6596 - val_accuracy: 0.8771\n",
      "Epoch 94/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1735 - accuracy: 0.9620 - val_loss: 0.6849 - val_accuracy: 0.8535\n",
      "Epoch 95/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1725 - accuracy: 0.9610 - val_loss: 0.6430 - val_accuracy: 0.8700\n",
      "Epoch 96/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1672 - accuracy: 0.9628 - val_loss: 0.6731 - val_accuracy: 0.8594\n",
      "Epoch 97/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1673 - accuracy: 0.9616 - val_loss: 0.6561 - val_accuracy: 0.8731\n",
      "Epoch 98/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1721 - accuracy: 0.9620 - val_loss: 0.6678 - val_accuracy: 0.8712\n",
      "Epoch 99/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1679 - accuracy: 0.9618 - val_loss: 0.6330 - val_accuracy: 0.8743\n",
      "Epoch 100/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1685 - accuracy: 0.9618 - val_loss: 0.6619 - val_accuracy: 0.8707\n",
      "Epoch 101/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1668 - accuracy: 0.9631 - val_loss: 0.6468 - val_accuracy: 0.8653\n",
      "Epoch 102/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1664 - accuracy: 0.9620 - val_loss: 0.7075 - val_accuracy: 0.8554\n",
      "Epoch 103/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1623 - accuracy: 0.9625 - val_loss: 0.6929 - val_accuracy: 0.8627\n",
      "Epoch 104/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1619 - accuracy: 0.9635 - val_loss: 0.6763 - val_accuracy: 0.8655\n",
      "Epoch 105/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1618 - accuracy: 0.9643 - val_loss: 0.6786 - val_accuracy: 0.8667\n",
      "Epoch 106/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1669 - accuracy: 0.9614 - val_loss: 0.6739 - val_accuracy: 0.8651\n",
      "Epoch 107/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1647 - accuracy: 0.9631 - val_loss: 0.7403 - val_accuracy: 0.8466\n",
      "Epoch 108/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1618 - accuracy: 0.9635 - val_loss: 0.6472 - val_accuracy: 0.8743\n",
      "Epoch 109/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1607 - accuracy: 0.9620 - val_loss: 0.6670 - val_accuracy: 0.8757\n",
      "Epoch 110/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1610 - accuracy: 0.9625 - val_loss: 0.6798 - val_accuracy: 0.8596\n",
      "Epoch 111/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1613 - accuracy: 0.9627 - val_loss: 0.7013 - val_accuracy: 0.8546\n",
      "Epoch 112/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1594 - accuracy: 0.9635 - val_loss: 0.6973 - val_accuracy: 0.8530\n",
      "Epoch 113/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1595 - accuracy: 0.9624 - val_loss: 0.6498 - val_accuracy: 0.8745\n",
      "Epoch 114/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1587 - accuracy: 0.9628 - val_loss: 0.7032 - val_accuracy: 0.8594\n",
      "Epoch 115/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1553 - accuracy: 0.9647 - val_loss: 0.6553 - val_accuracy: 0.8667\n",
      "Epoch 116/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1577 - accuracy: 0.9627 - val_loss: 0.7224 - val_accuracy: 0.8549\n",
      "Epoch 117/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1579 - accuracy: 0.9638 - val_loss: 0.7063 - val_accuracy: 0.8641\n",
      "Epoch 118/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1592 - accuracy: 0.9610 - val_loss: 0.7004 - val_accuracy: 0.8670\n",
      "Epoch 119/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1543 - accuracy: 0.9643 - val_loss: 0.7046 - val_accuracy: 0.8596\n",
      "Epoch 120/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1538 - accuracy: 0.9648 - val_loss: 0.7065 - val_accuracy: 0.8589\n",
      "Epoch 121/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1525 - accuracy: 0.9650 - val_loss: 0.6658 - val_accuracy: 0.8767\n",
      "Epoch 122/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1576 - accuracy: 0.9624 - val_loss: 0.7011 - val_accuracy: 0.8575\n",
      "Epoch 123/2000\n",
      "12672/12672 [==============================] - 0s 30us/sample - loss: 0.1518 - accuracy: 0.9657 - val_loss: 0.6420 - val_accuracy: 0.8804\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14482 samples, validate on 4827 samples\n",
      "Epoch 1/2000\n",
      "14482/14482 [==============================] - 1s 52us/sample - loss: 2.2329 - accuracy: 0.2999 - val_loss: 1.7627 - val_accuracy: 0.6563\n",
      "Epoch 2/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 1.4996 - accuracy: 0.5673 - val_loss: 1.2142 - val_accuracy: 0.7725\n",
      "Epoch 3/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 1.0836 - accuracy: 0.7063 - val_loss: 0.9321 - val_accuracy: 0.7976\n",
      "Epoch 4/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.8511 - accuracy: 0.7770 - val_loss: 0.7882 - val_accuracy: 0.8154\n",
      "Epoch 5/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.7127 - accuracy: 0.8167 - val_loss: 0.7211 - val_accuracy: 0.8080\n",
      "Epoch 6/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.6237 - accuracy: 0.8385 - val_loss: 0.6716 - val_accuracy: 0.8144\n",
      "Epoch 7/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.5592 - accuracy: 0.8591 - val_loss: 0.6531 - val_accuracy: 0.8057\n",
      "Epoch 8/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.5092 - accuracy: 0.8734 - val_loss: 0.6470 - val_accuracy: 0.8164\n",
      "Epoch 9/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.4816 - accuracy: 0.8819 - val_loss: 0.6497 - val_accuracy: 0.8135\n",
      "Epoch 10/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.4520 - accuracy: 0.8898 - val_loss: 0.6405 - val_accuracy: 0.8133\n",
      "Epoch 11/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.4254 - accuracy: 0.8982 - val_loss: 0.6517 - val_accuracy: 0.8208\n",
      "Epoch 12/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.4103 - accuracy: 0.8998 - val_loss: 0.6195 - val_accuracy: 0.8235\n",
      "Epoch 13/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3950 - accuracy: 0.9068 - val_loss: 0.6567 - val_accuracy: 0.8154\n",
      "Epoch 14/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.3846 - accuracy: 0.9082 - val_loss: 0.6559 - val_accuracy: 0.8231\n",
      "Epoch 15/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3736 - accuracy: 0.9120 - val_loss: 0.6634 - val_accuracy: 0.8281\n",
      "Epoch 16/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3624 - accuracy: 0.9153 - val_loss: 0.6708 - val_accuracy: 0.8254\n",
      "Epoch 17/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3506 - accuracy: 0.9163 - val_loss: 0.6762 - val_accuracy: 0.8231\n",
      "Epoch 18/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.3358 - accuracy: 0.9242 - val_loss: 0.6870 - val_accuracy: 0.8243\n",
      "Epoch 19/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3364 - accuracy: 0.9211 - val_loss: 0.6669 - val_accuracy: 0.8291\n",
      "Epoch 20/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3254 - accuracy: 0.9264 - val_loss: 0.6669 - val_accuracy: 0.8328\n",
      "Epoch 21/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3187 - accuracy: 0.9267 - val_loss: 0.6636 - val_accuracy: 0.8299\n",
      "Epoch 22/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3149 - accuracy: 0.9272 - val_loss: 0.6664 - val_accuracy: 0.8297\n",
      "Epoch 23/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3049 - accuracy: 0.9352 - val_loss: 0.6728 - val_accuracy: 0.8270\n",
      "Epoch 24/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.3024 - accuracy: 0.9305 - val_loss: 0.6763 - val_accuracy: 0.8268\n",
      "Epoch 25/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2998 - accuracy: 0.9303 - val_loss: 0.6682 - val_accuracy: 0.8241\n",
      "Epoch 26/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2876 - accuracy: 0.9343 - val_loss: 0.6886 - val_accuracy: 0.8330\n",
      "Epoch 27/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2878 - accuracy: 0.9310 - val_loss: 0.6755 - val_accuracy: 0.8299\n",
      "Epoch 28/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.2820 - accuracy: 0.9383 - val_loss: 0.6875 - val_accuracy: 0.8345\n",
      "Epoch 29/2000\n",
      "14482/14482 [==============================] - 0s 34us/sample - loss: 0.2780 - accuracy: 0.9381 - val_loss: 0.6911 - val_accuracy: 0.8361\n",
      "Epoch 30/2000\n",
      "14482/14482 [==============================] - 0s 34us/sample - loss: 0.2748 - accuracy: 0.9396 - val_loss: 0.6994 - val_accuracy: 0.8278\n",
      "Epoch 31/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.2729 - accuracy: 0.9383 - val_loss: 0.6930 - val_accuracy: 0.8339\n",
      "Epoch 32/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2713 - accuracy: 0.9367 - val_loss: 0.6882 - val_accuracy: 0.8322\n",
      "Epoch 33/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2591 - accuracy: 0.9419 - val_loss: 0.6817 - val_accuracy: 0.8355\n",
      "Epoch 34/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2595 - accuracy: 0.9405 - val_loss: 0.7017 - val_accuracy: 0.8353\n",
      "Epoch 35/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2587 - accuracy: 0.9405 - val_loss: 0.7057 - val_accuracy: 0.8320\n",
      "Epoch 36/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2467 - accuracy: 0.9452 - val_loss: 0.7164 - val_accuracy: 0.8380\n",
      "Epoch 37/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2468 - accuracy: 0.9441 - val_loss: 0.7015 - val_accuracy: 0.8403\n",
      "Epoch 38/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2461 - accuracy: 0.9454 - val_loss: 0.6778 - val_accuracy: 0.8428\n",
      "Epoch 39/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2408 - accuracy: 0.9459 - val_loss: 0.6968 - val_accuracy: 0.8390\n",
      "Epoch 40/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.2376 - accuracy: 0.9450 - val_loss: 0.6484 - val_accuracy: 0.8411\n",
      "Epoch 41/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2383 - accuracy: 0.9472 - val_loss: 0.7106 - val_accuracy: 0.8399\n",
      "Epoch 42/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.2372 - accuracy: 0.9450 - val_loss: 0.7005 - val_accuracy: 0.8384\n",
      "Epoch 43/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2308 - accuracy: 0.9470 - val_loss: 0.7168 - val_accuracy: 0.8368\n",
      "Epoch 44/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2324 - accuracy: 0.9466 - val_loss: 0.6869 - val_accuracy: 0.8500\n",
      "Epoch 45/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2300 - accuracy: 0.9461 - val_loss: 0.6655 - val_accuracy: 0.8380\n",
      "Epoch 46/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2294 - accuracy: 0.9448 - val_loss: 0.7024 - val_accuracy: 0.8452\n",
      "Epoch 47/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2272 - accuracy: 0.9460 - val_loss: 0.7198 - val_accuracy: 0.8341\n",
      "Epoch 48/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2171 - accuracy: 0.9510 - val_loss: 0.7054 - val_accuracy: 0.8423\n",
      "Epoch 49/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2212 - accuracy: 0.9484 - val_loss: 0.7126 - val_accuracy: 0.8411\n",
      "Epoch 50/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.2185 - accuracy: 0.9517 - val_loss: 0.6869 - val_accuracy: 0.8479\n",
      "Epoch 51/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2173 - accuracy: 0.9502 - val_loss: 0.7210 - val_accuracy: 0.8386\n",
      "Epoch 52/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2152 - accuracy: 0.9498 - val_loss: 0.7133 - val_accuracy: 0.8419\n",
      "Epoch 53/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.2176 - accuracy: 0.9479 - val_loss: 0.7339 - val_accuracy: 0.8488\n",
      "Epoch 54/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2117 - accuracy: 0.9501 - val_loss: 0.6975 - val_accuracy: 0.8434\n",
      "Epoch 55/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2110 - accuracy: 0.9513 - val_loss: 0.7209 - val_accuracy: 0.8446\n",
      "Epoch 56/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2073 - accuracy: 0.9524 - val_loss: 0.6945 - val_accuracy: 0.8469\n",
      "Epoch 57/2000\n",
      "14482/14482 [==============================] - 0s 32us/sample - loss: 0.2040 - accuracy: 0.9526 - val_loss: 0.7303 - val_accuracy: 0.8423\n",
      "Epoch 58/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2080 - accuracy: 0.9519 - val_loss: 0.7216 - val_accuracy: 0.8428\n",
      "Epoch 59/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2024 - accuracy: 0.9547 - val_loss: 0.7136 - val_accuracy: 0.8440\n",
      "Epoch 60/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2006 - accuracy: 0.9539 - val_loss: 0.7009 - val_accuracy: 0.8419\n",
      "Epoch 61/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2010 - accuracy: 0.9552 - val_loss: 0.7052 - val_accuracy: 0.8434\n",
      "Epoch 62/2000\n",
      "14482/14482 [==============================] - 0s 31us/sample - loss: 0.2007 - accuracy: 0.9525 - val_loss: 0.7468 - val_accuracy: 0.8452\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16292 samples, validate on 5431 samples\n",
      "Epoch 1/2000\n",
      "16292/16292 [==============================] - 1s 56us/sample - loss: 2.1212 - accuracy: 0.3632 - val_loss: 1.5803 - val_accuracy: 0.7529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 1.3639 - accuracy: 0.6150 - val_loss: 1.0477 - val_accuracy: 0.8549\n",
      "Epoch 3/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 1.0034 - accuracy: 0.7165 - val_loss: 0.7965 - val_accuracy: 0.8682\n",
      "Epoch 4/2000\n",
      "16292/16292 [==============================] - 0s 31us/sample - loss: 0.8081 - accuracy: 0.7788 - val_loss: 0.6501 - val_accuracy: 0.8938\n",
      "Epoch 5/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.6869 - accuracy: 0.8167 - val_loss: 0.5735 - val_accuracy: 0.8939\n",
      "Epoch 6/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.6046 - accuracy: 0.8408 - val_loss: 0.5173 - val_accuracy: 0.9068\n",
      "Epoch 7/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.5502 - accuracy: 0.8584 - val_loss: 0.4929 - val_accuracy: 0.9083\n",
      "Epoch 8/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.5110 - accuracy: 0.8692 - val_loss: 0.4649 - val_accuracy: 0.9175\n",
      "Epoch 9/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.4813 - accuracy: 0.8772 - val_loss: 0.4509 - val_accuracy: 0.9203\n",
      "Epoch 10/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.4552 - accuracy: 0.8849 - val_loss: 0.4413 - val_accuracy: 0.9190\n",
      "Epoch 11/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.4335 - accuracy: 0.8923 - val_loss: 0.4270 - val_accuracy: 0.9212\n",
      "Epoch 12/2000\n",
      "16292/16292 [==============================] - 0s 31us/sample - loss: 0.4200 - accuracy: 0.8945 - val_loss: 0.4135 - val_accuracy: 0.9300\n",
      "Epoch 13/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.4045 - accuracy: 0.9001 - val_loss: 0.4171 - val_accuracy: 0.9149\n",
      "Epoch 14/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3875 - accuracy: 0.9076 - val_loss: 0.3998 - val_accuracy: 0.9298\n",
      "Epoch 15/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3838 - accuracy: 0.9058 - val_loss: 0.3975 - val_accuracy: 0.9332\n",
      "Epoch 16/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3693 - accuracy: 0.9096 - val_loss: 0.3818 - val_accuracy: 0.9341\n",
      "Epoch 17/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3659 - accuracy: 0.9103 - val_loss: 0.3844 - val_accuracy: 0.9350\n",
      "Epoch 18/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3500 - accuracy: 0.9189 - val_loss: 0.3838 - val_accuracy: 0.9326\n",
      "Epoch 19/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.3466 - accuracy: 0.9161 - val_loss: 0.3762 - val_accuracy: 0.9361\n",
      "Epoch 20/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3380 - accuracy: 0.9224 - val_loss: 0.3798 - val_accuracy: 0.9391\n",
      "Epoch 21/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3300 - accuracy: 0.9219 - val_loss: 0.3786 - val_accuracy: 0.9398\n",
      "Epoch 22/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3254 - accuracy: 0.9219 - val_loss: 0.3634 - val_accuracy: 0.9394\n",
      "Epoch 23/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3191 - accuracy: 0.9244 - val_loss: 0.3718 - val_accuracy: 0.9413\n",
      "Epoch 24/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.3085 - accuracy: 0.9266 - val_loss: 0.3572 - val_accuracy: 0.9394\n",
      "Epoch 25/2000\n",
      "16292/16292 [==============================] - 0s 31us/sample - loss: 0.3073 - accuracy: 0.9271 - val_loss: 0.3624 - val_accuracy: 0.9413\n",
      "Epoch 26/2000\n",
      "16292/16292 [==============================] - 0s 31us/sample - loss: 0.2994 - accuracy: 0.9306 - val_loss: 0.3647 - val_accuracy: 0.9387\n",
      "Epoch 27/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.2989 - accuracy: 0.9314 - val_loss: 0.3527 - val_accuracy: 0.9400\n",
      "Epoch 28/2000\n",
      "16292/16292 [==============================] - 1s 33us/sample - loss: 0.2949 - accuracy: 0.9304 - val_loss: 0.3514 - val_accuracy: 0.9394\n",
      "Epoch 29/2000\n",
      "16292/16292 [==============================] - 1s 38us/sample - loss: 0.2884 - accuracy: 0.9335 - val_loss: 0.3528 - val_accuracy: 0.9411\n",
      "Epoch 30/2000\n",
      "16292/16292 [==============================] - 1s 39us/sample - loss: 0.2859 - accuracy: 0.9332 - val_loss: 0.3513 - val_accuracy: 0.9372\n",
      "Epoch 31/2000\n",
      "16292/16292 [==============================] - 1s 40us/sample - loss: 0.2879 - accuracy: 0.9301 - val_loss: 0.3398 - val_accuracy: 0.9427\n",
      "Epoch 32/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.2785 - accuracy: 0.9335 - val_loss: 0.3520 - val_accuracy: 0.9402\n",
      "Epoch 33/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2742 - accuracy: 0.9348 - val_loss: 0.3421 - val_accuracy: 0.9398\n",
      "Epoch 34/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2736 - accuracy: 0.9351 - val_loss: 0.3432 - val_accuracy: 0.9387\n",
      "Epoch 35/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2647 - accuracy: 0.9383 - val_loss: 0.3445 - val_accuracy: 0.9403\n",
      "Epoch 36/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.2657 - accuracy: 0.9372 - val_loss: 0.3397 - val_accuracy: 0.9438\n",
      "Epoch 37/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2627 - accuracy: 0.9374 - val_loss: 0.3482 - val_accuracy: 0.9414\n",
      "Epoch 38/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2614 - accuracy: 0.9386 - val_loss: 0.3316 - val_accuracy: 0.9461\n",
      "Epoch 39/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2521 - accuracy: 0.9418 - val_loss: 0.3353 - val_accuracy: 0.9416\n",
      "Epoch 40/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2555 - accuracy: 0.9376 - val_loss: 0.3351 - val_accuracy: 0.9422\n",
      "Epoch 41/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2553 - accuracy: 0.9368 - val_loss: 0.3323 - val_accuracy: 0.9405\n",
      "Epoch 42/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2482 - accuracy: 0.9397 - val_loss: 0.3308 - val_accuracy: 0.9427\n",
      "Epoch 43/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2498 - accuracy: 0.9397 - val_loss: 0.3244 - val_accuracy: 0.9449\n",
      "Epoch 44/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.2414 - accuracy: 0.9432 - val_loss: 0.3321 - val_accuracy: 0.9387\n",
      "Epoch 45/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2406 - accuracy: 0.9432 - val_loss: 0.3253 - val_accuracy: 0.9422\n",
      "Epoch 46/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2355 - accuracy: 0.9435 - val_loss: 0.3243 - val_accuracy: 0.9424\n",
      "Epoch 47/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.2340 - accuracy: 0.9446 - val_loss: 0.3221 - val_accuracy: 0.9457\n",
      "Epoch 48/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2308 - accuracy: 0.9470 - val_loss: 0.3273 - val_accuracy: 0.9413\n",
      "Epoch 49/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2296 - accuracy: 0.9448 - val_loss: 0.3223 - val_accuracy: 0.9405\n",
      "Epoch 50/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2313 - accuracy: 0.9443 - val_loss: 0.3211 - val_accuracy: 0.9433\n",
      "Epoch 51/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2260 - accuracy: 0.9460 - val_loss: 0.3195 - val_accuracy: 0.9440\n",
      "Epoch 52/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2273 - accuracy: 0.9445 - val_loss: 0.3170 - val_accuracy: 0.9455\n",
      "Epoch 53/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2227 - accuracy: 0.9489 - val_loss: 0.3171 - val_accuracy: 0.9457\n",
      "Epoch 54/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2255 - accuracy: 0.9459 - val_loss: 0.3180 - val_accuracy: 0.9462\n",
      "Epoch 55/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2205 - accuracy: 0.9481 - val_loss: 0.3119 - val_accuracy: 0.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2214 - accuracy: 0.9462 - val_loss: 0.3298 - val_accuracy: 0.9381\n",
      "Epoch 57/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2189 - accuracy: 0.9457 - val_loss: 0.3161 - val_accuracy: 0.9446\n",
      "Epoch 58/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2166 - accuracy: 0.9465 - val_loss: 0.3117 - val_accuracy: 0.9475\n",
      "Epoch 59/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2151 - accuracy: 0.9479 - val_loss: 0.3153 - val_accuracy: 0.9427\n",
      "Epoch 60/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2116 - accuracy: 0.9495 - val_loss: 0.3124 - val_accuracy: 0.9418\n",
      "Epoch 61/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2172 - accuracy: 0.9465 - val_loss: 0.3070 - val_accuracy: 0.9431\n",
      "Epoch 62/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2121 - accuracy: 0.9483 - val_loss: 0.3080 - val_accuracy: 0.9451\n",
      "Epoch 63/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2087 - accuracy: 0.9517 - val_loss: 0.3236 - val_accuracy: 0.9418\n",
      "Epoch 64/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2097 - accuracy: 0.9493 - val_loss: 0.3138 - val_accuracy: 0.9427\n",
      "Epoch 65/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2041 - accuracy: 0.9524 - val_loss: 0.3095 - val_accuracy: 0.9431\n",
      "Epoch 66/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2070 - accuracy: 0.9502 - val_loss: 0.3070 - val_accuracy: 0.9468\n",
      "Epoch 67/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2040 - accuracy: 0.9514 - val_loss: 0.3060 - val_accuracy: 0.9459\n",
      "Epoch 68/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.2026 - accuracy: 0.9508 - val_loss: 0.3106 - val_accuracy: 0.9481\n",
      "Epoch 69/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.3126 - val_accuracy: 0.9429\n",
      "Epoch 70/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2005 - accuracy: 0.9499 - val_loss: 0.3046 - val_accuracy: 0.9490\n",
      "Epoch 71/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1998 - accuracy: 0.9510 - val_loss: 0.2992 - val_accuracy: 0.9481\n",
      "Epoch 72/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.2041 - accuracy: 0.9489 - val_loss: 0.2981 - val_accuracy: 0.9455\n",
      "Epoch 73/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1984 - accuracy: 0.9521 - val_loss: 0.3020 - val_accuracy: 0.9438\n",
      "Epoch 74/2000\n",
      "16292/16292 [==============================] - 1s 38us/sample - loss: 0.1969 - accuracy: 0.9534 - val_loss: 0.2959 - val_accuracy: 0.9490\n",
      "Epoch 75/2000\n",
      "16292/16292 [==============================] - 1s 34us/sample - loss: 0.2001 - accuracy: 0.9492 - val_loss: 0.3026 - val_accuracy: 0.9444\n",
      "Epoch 76/2000\n",
      "16292/16292 [==============================] - 1s 34us/sample - loss: 0.1949 - accuracy: 0.9513 - val_loss: 0.3058 - val_accuracy: 0.9413\n",
      "Epoch 77/2000\n",
      "16292/16292 [==============================] - 1s 33us/sample - loss: 0.1938 - accuracy: 0.9529 - val_loss: 0.3051 - val_accuracy: 0.9453\n",
      "Epoch 78/2000\n",
      "16292/16292 [==============================] - 1s 34us/sample - loss: 0.1990 - accuracy: 0.9525 - val_loss: 0.2993 - val_accuracy: 0.9457\n",
      "Epoch 79/2000\n",
      "16292/16292 [==============================] - 1s 35us/sample - loss: 0.1963 - accuracy: 0.9535 - val_loss: 0.3044 - val_accuracy: 0.9422\n",
      "Epoch 80/2000\n",
      "16292/16292 [==============================] - 1s 40us/sample - loss: 0.1910 - accuracy: 0.9532 - val_loss: 0.2951 - val_accuracy: 0.9459\n",
      "Epoch 81/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1867 - accuracy: 0.9530 - val_loss: 0.3048 - val_accuracy: 0.9431\n",
      "Epoch 82/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1891 - accuracy: 0.9540 - val_loss: 0.2993 - val_accuracy: 0.9475\n",
      "Epoch 83/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1919 - accuracy: 0.9516 - val_loss: 0.2934 - val_accuracy: 0.9477\n",
      "Epoch 84/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1855 - accuracy: 0.9556 - val_loss: 0.2987 - val_accuracy: 0.9442\n",
      "Epoch 85/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1892 - accuracy: 0.9529 - val_loss: 0.2973 - val_accuracy: 0.9429\n",
      "Epoch 86/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1850 - accuracy: 0.9553 - val_loss: 0.2995 - val_accuracy: 0.9461\n",
      "Epoch 87/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1832 - accuracy: 0.9537 - val_loss: 0.3008 - val_accuracy: 0.9427\n",
      "Epoch 88/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1844 - accuracy: 0.9516 - val_loss: 0.2939 - val_accuracy: 0.9448\n",
      "Epoch 89/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1831 - accuracy: 0.9555 - val_loss: 0.3007 - val_accuracy: 0.9477\n",
      "Epoch 90/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1857 - accuracy: 0.9553 - val_loss: 0.2995 - val_accuracy: 0.9451\n",
      "Epoch 91/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1813 - accuracy: 0.9559 - val_loss: 0.3061 - val_accuracy: 0.9429\n",
      "Epoch 92/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1827 - accuracy: 0.9560 - val_loss: 0.2956 - val_accuracy: 0.9459\n",
      "Epoch 93/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1820 - accuracy: 0.9538 - val_loss: 0.2974 - val_accuracy: 0.9472\n",
      "Epoch 94/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1835 - accuracy: 0.9535 - val_loss: 0.3112 - val_accuracy: 0.9459\n",
      "Epoch 95/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1761 - accuracy: 0.9576 - val_loss: 0.3000 - val_accuracy: 0.9437\n",
      "Epoch 96/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1775 - accuracy: 0.9570 - val_loss: 0.3035 - val_accuracy: 0.9426\n",
      "Epoch 97/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1810 - accuracy: 0.9530 - val_loss: 0.2949 - val_accuracy: 0.9464\n",
      "Epoch 98/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1752 - accuracy: 0.9565 - val_loss: 0.2989 - val_accuracy: 0.9448\n",
      "Epoch 99/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1778 - accuracy: 0.9549 - val_loss: 0.3014 - val_accuracy: 0.9440\n",
      "Epoch 100/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1761 - accuracy: 0.9576 - val_loss: 0.2887 - val_accuracy: 0.9473\n",
      "Epoch 101/2000\n",
      "16292/16292 [==============================] - 1s 34us/sample - loss: 0.1759 - accuracy: 0.9566 - val_loss: 0.2960 - val_accuracy: 0.9448\n",
      "Epoch 102/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1742 - accuracy: 0.9565 - val_loss: 0.2957 - val_accuracy: 0.9437\n",
      "Epoch 103/2000\n",
      "16292/16292 [==============================] - 1s 35us/sample - loss: 0.1773 - accuracy: 0.9546 - val_loss: 0.2905 - val_accuracy: 0.9479\n",
      "Epoch 104/2000\n",
      "16292/16292 [==============================] - 1s 35us/sample - loss: 0.1712 - accuracy: 0.9586 - val_loss: 0.2912 - val_accuracy: 0.9473\n",
      "Epoch 105/2000\n",
      "16292/16292 [==============================] - 1s 35us/sample - loss: 0.1758 - accuracy: 0.9573 - val_loss: 0.3013 - val_accuracy: 0.9424\n",
      "Epoch 106/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1754 - accuracy: 0.9572 - val_loss: 0.3086 - val_accuracy: 0.9378\n",
      "Epoch 107/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1730 - accuracy: 0.9568 - val_loss: 0.2926 - val_accuracy: 0.9449\n",
      "Epoch 108/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1690 - accuracy: 0.9570 - val_loss: 0.2941 - val_accuracy: 0.9448\n",
      "Epoch 109/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1677 - accuracy: 0.9576 - val_loss: 0.2842 - val_accuracy: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1703 - accuracy: 0.9588 - val_loss: 0.2961 - val_accuracy: 0.9427\n",
      "Epoch 111/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1717 - accuracy: 0.9565 - val_loss: 0.2911 - val_accuracy: 0.9431\n",
      "Epoch 112/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1693 - accuracy: 0.9570 - val_loss: 0.3087 - val_accuracy: 0.9426\n",
      "Epoch 113/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1708 - accuracy: 0.9589 - val_loss: 0.3053 - val_accuracy: 0.9427\n",
      "Epoch 114/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1666 - accuracy: 0.9587 - val_loss: 0.3049 - val_accuracy: 0.9426\n",
      "Epoch 115/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1672 - accuracy: 0.9593 - val_loss: 0.2928 - val_accuracy: 0.9481\n",
      "Epoch 116/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1669 - accuracy: 0.9570 - val_loss: 0.3077 - val_accuracy: 0.9400\n",
      "Epoch 117/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1696 - accuracy: 0.9568 - val_loss: 0.2912 - val_accuracy: 0.9475\n",
      "Epoch 118/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1654 - accuracy: 0.9588 - val_loss: 0.2981 - val_accuracy: 0.9437\n",
      "Epoch 119/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1624 - accuracy: 0.9603 - val_loss: 0.3021 - val_accuracy: 0.9431\n",
      "Epoch 120/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1658 - accuracy: 0.9581 - val_loss: 0.2981 - val_accuracy: 0.9426\n",
      "Epoch 121/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1645 - accuracy: 0.9587 - val_loss: 0.2903 - val_accuracy: 0.9464\n",
      "Epoch 122/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1690 - accuracy: 0.9568 - val_loss: 0.2927 - val_accuracy: 0.9414\n",
      "Epoch 123/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1657 - accuracy: 0.9591 - val_loss: 0.2954 - val_accuracy: 0.9446\n",
      "Epoch 124/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1551 - accuracy: 0.9633 - val_loss: 0.2946 - val_accuracy: 0.9429\n",
      "Epoch 125/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1616 - accuracy: 0.9586 - val_loss: 0.3040 - val_accuracy: 0.9411\n",
      "Epoch 126/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1601 - accuracy: 0.9609 - val_loss: 0.2991 - val_accuracy: 0.9422\n",
      "Epoch 127/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1602 - accuracy: 0.9592 - val_loss: 0.2931 - val_accuracy: 0.9414\n",
      "Epoch 128/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1600 - accuracy: 0.9597 - val_loss: 0.3037 - val_accuracy: 0.9398\n",
      "Epoch 129/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1603 - accuracy: 0.9613 - val_loss: 0.2918 - val_accuracy: 0.9402\n",
      "Epoch 130/2000\n",
      "16292/16292 [==============================] - 1s 33us/sample - loss: 0.1573 - accuracy: 0.9603 - val_loss: 0.3000 - val_accuracy: 0.9416\n",
      "Epoch 131/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1612 - accuracy: 0.9590 - val_loss: 0.3019 - val_accuracy: 0.9402\n",
      "Epoch 132/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1596 - accuracy: 0.9605 - val_loss: 0.3000 - val_accuracy: 0.9383\n",
      "Epoch 133/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1548 - accuracy: 0.9628 - val_loss: 0.3005 - val_accuracy: 0.9422\n",
      "Epoch 134/2000\n",
      "16292/16292 [==============================] - 1s 33us/sample - loss: 0.1636 - accuracy: 0.9577 - val_loss: 0.2928 - val_accuracy: 0.9466\n",
      "Epoch 135/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1587 - accuracy: 0.9611 - val_loss: 0.2892 - val_accuracy: 0.9449\n",
      "Epoch 136/2000\n",
      "16292/16292 [==============================] - 1s 34us/sample - loss: 0.1568 - accuracy: 0.9605 - val_loss: 0.2922 - val_accuracy: 0.9453\n",
      "Epoch 137/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1575 - accuracy: 0.9609 - val_loss: 0.2952 - val_accuracy: 0.9394\n",
      "Epoch 138/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1593 - accuracy: 0.9581 - val_loss: 0.2994 - val_accuracy: 0.9420\n",
      "Epoch 139/2000\n",
      "16292/16292 [==============================] - 1s 36us/sample - loss: 0.1571 - accuracy: 0.9619 - val_loss: 0.2825 - val_accuracy: 0.9473\n",
      "Epoch 140/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1586 - accuracy: 0.9603 - val_loss: 0.2933 - val_accuracy: 0.9418\n",
      "Epoch 141/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1564 - accuracy: 0.9608 - val_loss: 0.2875 - val_accuracy: 0.9438\n",
      "Epoch 142/2000\n",
      "16292/16292 [==============================] - 1s 34us/sample - loss: 0.1535 - accuracy: 0.9613 - val_loss: 0.2829 - val_accuracy: 0.9473\n",
      "Epoch 143/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1545 - accuracy: 0.9610 - val_loss: 0.2954 - val_accuracy: 0.9416\n",
      "Epoch 144/2000\n",
      "16292/16292 [==============================] - 1s 34us/sample - loss: 0.1548 - accuracy: 0.9604 - val_loss: 0.2890 - val_accuracy: 0.9444\n",
      "Epoch 145/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1564 - accuracy: 0.9588 - val_loss: 0.2938 - val_accuracy: 0.9427\n",
      "Epoch 146/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1534 - accuracy: 0.9619 - val_loss: 0.2978 - val_accuracy: 0.9438\n",
      "Epoch 147/2000\n",
      "16292/16292 [==============================] - 1s 35us/sample - loss: 0.1544 - accuracy: 0.9594 - val_loss: 0.2931 - val_accuracy: 0.9379\n",
      "Epoch 148/2000\n",
      "16292/16292 [==============================] - 1s 37us/sample - loss: 0.1555 - accuracy: 0.9584 - val_loss: 0.2998 - val_accuracy: 0.9405\n",
      "Epoch 149/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1548 - accuracy: 0.9603 - val_loss: 0.3060 - val_accuracy: 0.9376\n",
      "Epoch 150/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1510 - accuracy: 0.9626 - val_loss: 0.2900 - val_accuracy: 0.9424\n",
      "Epoch 151/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1511 - accuracy: 0.9633 - val_loss: 0.2954 - val_accuracy: 0.9403\n",
      "Epoch 152/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1525 - accuracy: 0.9606 - val_loss: 0.2956 - val_accuracy: 0.9409\n",
      "Epoch 153/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1512 - accuracy: 0.9614 - val_loss: 0.2966 - val_accuracy: 0.9424\n",
      "Epoch 154/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1481 - accuracy: 0.9624 - val_loss: 0.2997 - val_accuracy: 0.9398\n",
      "Epoch 155/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1472 - accuracy: 0.9621 - val_loss: 0.2825 - val_accuracy: 0.9438\n",
      "Epoch 156/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1531 - accuracy: 0.9591 - val_loss: 0.2905 - val_accuracy: 0.9437\n",
      "Epoch 157/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1517 - accuracy: 0.9603 - val_loss: 0.2916 - val_accuracy: 0.9461\n",
      "Epoch 158/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1509 - accuracy: 0.9612 - val_loss: 0.2872 - val_accuracy: 0.9424\n",
      "Epoch 159/2000\n",
      "16292/16292 [==============================] - 1s 35us/sample - loss: 0.1495 - accuracy: 0.9606 - val_loss: 0.2952 - val_accuracy: 0.9424\n",
      "Epoch 160/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1558 - accuracy: 0.9593 - val_loss: 0.2838 - val_accuracy: 0.9470\n",
      "Epoch 161/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1491 - accuracy: 0.9608 - val_loss: 0.3012 - val_accuracy: 0.9438\n",
      "Epoch 162/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1460 - accuracy: 0.9640 - val_loss: 0.2840 - val_accuracy: 0.9481\n",
      "Epoch 163/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1510 - accuracy: 0.9609 - val_loss: 0.2846 - val_accuracy: 0.9484\n",
      "Epoch 164/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1504 - accuracy: 0.9604 - val_loss: 0.2968 - val_accuracy: 0.9398\n",
      "Epoch 165/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1487 - accuracy: 0.9617 - val_loss: 0.2867 - val_accuracy: 0.9457\n",
      "Epoch 166/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1506 - accuracy: 0.9594 - val_loss: 0.2826 - val_accuracy: 0.9490\n",
      "Epoch 167/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1498 - accuracy: 0.9611 - val_loss: 0.2860 - val_accuracy: 0.9431\n",
      "Epoch 168/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1434 - accuracy: 0.9639 - val_loss: 0.2983 - val_accuracy: 0.9464\n",
      "Epoch 169/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1468 - accuracy: 0.9623 - val_loss: 0.3005 - val_accuracy: 0.9429\n",
      "Epoch 170/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1477 - accuracy: 0.9615 - val_loss: 0.2972 - val_accuracy: 0.9418\n",
      "Epoch 171/2000\n",
      "16292/16292 [==============================] - 1s 33us/sample - loss: 0.1441 - accuracy: 0.9635 - val_loss: 0.2862 - val_accuracy: 0.9438\n",
      "Epoch 172/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1465 - accuracy: 0.9629 - val_loss: 0.2892 - val_accuracy: 0.9431\n",
      "Epoch 173/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1473 - accuracy: 0.9630 - val_loss: 0.2884 - val_accuracy: 0.9505\n",
      "Epoch 174/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1466 - accuracy: 0.9636 - val_loss: 0.2914 - val_accuracy: 0.9438\n",
      "Epoch 175/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1443 - accuracy: 0.9621 - val_loss: 0.2906 - val_accuracy: 0.9446\n",
      "Epoch 176/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1471 - accuracy: 0.9613 - val_loss: 0.2974 - val_accuracy: 0.9402\n",
      "Epoch 177/2000\n",
      "16292/16292 [==============================] - 1s 37us/sample - loss: 0.1382 - accuracy: 0.9640 - val_loss: 0.2836 - val_accuracy: 0.9479\n",
      "Epoch 178/2000\n",
      "16292/16292 [==============================] - 1s 34us/sample - loss: 0.1456 - accuracy: 0.9623 - val_loss: 0.2915 - val_accuracy: 0.9453\n",
      "Epoch 179/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1433 - accuracy: 0.9635 - val_loss: 0.2778 - val_accuracy: 0.9462\n",
      "Epoch 180/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1429 - accuracy: 0.9631 - val_loss: 0.2862 - val_accuracy: 0.9461\n",
      "Epoch 181/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1451 - accuracy: 0.9632 - val_loss: 0.2879 - val_accuracy: 0.9462\n",
      "Epoch 182/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1412 - accuracy: 0.9631 - val_loss: 0.3050 - val_accuracy: 0.9409\n",
      "Epoch 183/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1439 - accuracy: 0.9626 - val_loss: 0.2928 - val_accuracy: 0.9433\n",
      "Epoch 184/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1451 - accuracy: 0.9621 - val_loss: 0.2872 - val_accuracy: 0.9431\n",
      "Epoch 185/2000\n",
      "16292/16292 [==============================] - 1s 31us/sample - loss: 0.1438 - accuracy: 0.9612 - val_loss: 0.2845 - val_accuracy: 0.9422\n",
      "Epoch 186/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1392 - accuracy: 0.9618 - val_loss: 0.2881 - val_accuracy: 0.9411\n",
      "Epoch 187/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1449 - accuracy: 0.9618 - val_loss: 0.2949 - val_accuracy: 0.9455\n",
      "Epoch 188/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1371 - accuracy: 0.9653 - val_loss: 0.2796 - val_accuracy: 0.9483\n",
      "Epoch 189/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1471 - accuracy: 0.9607 - val_loss: 0.2944 - val_accuracy: 0.9449\n",
      "Epoch 190/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1448 - accuracy: 0.9605 - val_loss: 0.3005 - val_accuracy: 0.9361\n",
      "Epoch 191/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1372 - accuracy: 0.9650 - val_loss: 0.2833 - val_accuracy: 0.9448\n",
      "Epoch 192/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1396 - accuracy: 0.9647 - val_loss: 0.2874 - val_accuracy: 0.9438\n",
      "Epoch 193/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1369 - accuracy: 0.9641 - val_loss: 0.2812 - val_accuracy: 0.9448\n",
      "Epoch 194/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1398 - accuracy: 0.9637 - val_loss: 0.2906 - val_accuracy: 0.9391\n",
      "Epoch 195/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1414 - accuracy: 0.9634 - val_loss: 0.2845 - val_accuracy: 0.9424\n",
      "Epoch 196/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1407 - accuracy: 0.9631 - val_loss: 0.2777 - val_accuracy: 0.9464\n",
      "Epoch 197/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1382 - accuracy: 0.9633 - val_loss: 0.2760 - val_accuracy: 0.9433\n",
      "Epoch 198/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1371 - accuracy: 0.9643 - val_loss: 0.2821 - val_accuracy: 0.9446\n",
      "Epoch 199/2000\n",
      "16292/16292 [==============================] - 1s 32us/sample - loss: 0.1350 - accuracy: 0.9641 - val_loss: 0.2854 - val_accuracy: 0.9484\n",
      "Epoch 200/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1407 - accuracy: 0.9611 - val_loss: 0.2902 - val_accuracy: 0.9446\n",
      "Epoch 201/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1387 - accuracy: 0.9643 - val_loss: 0.2868 - val_accuracy: 0.9435\n",
      "Epoch 202/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1393 - accuracy: 0.9644 - val_loss: 0.2937 - val_accuracy: 0.9446\n",
      "Epoch 203/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1367 - accuracy: 0.9650 - val_loss: 0.3053 - val_accuracy: 0.9372\n",
      "Epoch 204/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1387 - accuracy: 0.9640 - val_loss: 0.2919 - val_accuracy: 0.9389\n",
      "Epoch 205/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1330 - accuracy: 0.9654 - val_loss: 0.2919 - val_accuracy: 0.9422\n",
      "Epoch 206/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1387 - accuracy: 0.9620 - val_loss: 0.2912 - val_accuracy: 0.9472\n",
      "Epoch 207/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1390 - accuracy: 0.9626 - val_loss: 0.2858 - val_accuracy: 0.9464\n",
      "Epoch 208/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1380 - accuracy: 0.9631 - val_loss: 0.2936 - val_accuracy: 0.9418\n",
      "Epoch 209/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1316 - accuracy: 0.9657 - val_loss: 0.2866 - val_accuracy: 0.9457\n",
      "Epoch 210/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1386 - accuracy: 0.9626 - val_loss: 0.2929 - val_accuracy: 0.9398\n",
      "Epoch 211/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1341 - accuracy: 0.9642 - val_loss: 0.2822 - val_accuracy: 0.9459\n",
      "Epoch 212/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1394 - accuracy: 0.9632 - val_loss: 0.2972 - val_accuracy: 0.9398\n",
      "Epoch 213/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1386 - accuracy: 0.9632 - val_loss: 0.3007 - val_accuracy: 0.9437\n",
      "Epoch 214/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1365 - accuracy: 0.9628 - val_loss: 0.3113 - val_accuracy: 0.9379\n",
      "Epoch 215/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1364 - accuracy: 0.9632 - val_loss: 0.3180 - val_accuracy: 0.9352\n",
      "Epoch 216/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1346 - accuracy: 0.9651 - val_loss: 0.3070 - val_accuracy: 0.9370\n",
      "Epoch 217/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1348 - accuracy: 0.9647 - val_loss: 0.3013 - val_accuracy: 0.9400\n",
      "Epoch 218/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1402 - accuracy: 0.9619 - val_loss: 0.2955 - val_accuracy: 0.9414\n",
      "Epoch 219/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1335 - accuracy: 0.9662 - val_loss: 0.3026 - val_accuracy: 0.9343\n",
      "Epoch 220/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1361 - accuracy: 0.9643 - val_loss: 0.2985 - val_accuracy: 0.9392\n",
      "Epoch 221/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1339 - accuracy: 0.9655 - val_loss: 0.2937 - val_accuracy: 0.9429\n",
      "Epoch 222/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1344 - accuracy: 0.9654 - val_loss: 0.2958 - val_accuracy: 0.9427\n",
      "Epoch 223/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1355 - accuracy: 0.9646 - val_loss: 0.2847 - val_accuracy: 0.9444\n",
      "Epoch 224/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1308 - accuracy: 0.9664 - val_loss: 0.2919 - val_accuracy: 0.9431\n",
      "Epoch 225/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1369 - accuracy: 0.9630 - val_loss: 0.2941 - val_accuracy: 0.9418\n",
      "Epoch 226/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1371 - accuracy: 0.9650 - val_loss: 0.2792 - val_accuracy: 0.9473\n",
      "Epoch 227/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1329 - accuracy: 0.9660 - val_loss: 0.2824 - val_accuracy: 0.9429\n",
      "Epoch 228/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1298 - accuracy: 0.9665 - val_loss: 0.2930 - val_accuracy: 0.9446\n",
      "Epoch 229/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1314 - accuracy: 0.9640 - val_loss: 0.3106 - val_accuracy: 0.9339\n",
      "Epoch 230/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1282 - accuracy: 0.9669 - val_loss: 0.2981 - val_accuracy: 0.9407\n",
      "Epoch 231/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1313 - accuracy: 0.9640 - val_loss: 0.2947 - val_accuracy: 0.9383\n",
      "Epoch 232/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1298 - accuracy: 0.9647 - val_loss: 0.3020 - val_accuracy: 0.9391\n",
      "Epoch 233/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1372 - accuracy: 0.9637 - val_loss: 0.3001 - val_accuracy: 0.9378\n",
      "Epoch 234/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1285 - accuracy: 0.9653 - val_loss: 0.3052 - val_accuracy: 0.9367\n",
      "Epoch 235/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1296 - accuracy: 0.9638 - val_loss: 0.2943 - val_accuracy: 0.9394\n",
      "Epoch 236/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1295 - accuracy: 0.9654 - val_loss: 0.3000 - val_accuracy: 0.9402\n",
      "Epoch 237/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1306 - accuracy: 0.9645 - val_loss: 0.2865 - val_accuracy: 0.9481\n",
      "Epoch 238/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1323 - accuracy: 0.9642 - val_loss: 0.3023 - val_accuracy: 0.9411\n",
      "Epoch 239/2000\n",
      "16292/16292 [==============================] - 1s 36us/sample - loss: 0.1353 - accuracy: 0.9643 - val_loss: 0.2869 - val_accuracy: 0.9442\n",
      "Epoch 240/2000\n",
      "16292/16292 [==============================] - 1s 38us/sample - loss: 0.1348 - accuracy: 0.9646 - val_loss: 0.3060 - val_accuracy: 0.9396\n",
      "Epoch 241/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1326 - accuracy: 0.9654 - val_loss: 0.3009 - val_accuracy: 0.9402\n",
      "Epoch 242/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1318 - accuracy: 0.9647 - val_loss: 0.2915 - val_accuracy: 0.9427\n",
      "Epoch 243/2000\n",
      "16292/16292 [==============================] - 0s 30us/sample - loss: 0.1257 - accuracy: 0.9663 - val_loss: 0.2833 - val_accuracy: 0.9466\n",
      "Epoch 244/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1308 - accuracy: 0.9656 - val_loss: 0.3039 - val_accuracy: 0.9413\n",
      "Epoch 245/2000\n",
      "16292/16292 [==============================] - 0s 29us/sample - loss: 0.1333 - accuracy: 0.9646 - val_loss: 0.2775 - val_accuracy: 0.9468\n",
      "Epoch 246/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1303 - accuracy: 0.9662 - val_loss: 0.2935 - val_accuracy: 0.9420\n",
      "Epoch 247/2000\n",
      "16292/16292 [==============================] - 0s 28us/sample - loss: 0.1354 - accuracy: 0.9637 - val_loss: 0.2919 - val_accuracy: 0.9451\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 18102 samples, validate on 6034 samples\n",
      "Epoch 1/2000\n",
      "18102/18102 [==============================] - 1s 45us/sample - loss: 2.0817 - accuracy: 0.3569 - val_loss: 1.5009 - val_accuracy: 0.8088\n",
      "Epoch 2/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 1.3043 - accuracy: 0.6214 - val_loss: 0.9693 - val_accuracy: 0.8195\n",
      "Epoch 3/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.9376 - accuracy: 0.7377 - val_loss: 0.7253 - val_accuracy: 0.8881\n",
      "Epoch 4/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.7446 - accuracy: 0.8006 - val_loss: 0.5981 - val_accuracy: 0.9118\n",
      "Epoch 5/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.6261 - accuracy: 0.8351 - val_loss: 0.5309 - val_accuracy: 0.9155\n",
      "Epoch 6/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.5530 - accuracy: 0.8554 - val_loss: 0.4950 - val_accuracy: 0.9168\n",
      "Epoch 7/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.5054 - accuracy: 0.8731 - val_loss: 0.4694 - val_accuracy: 0.9226\n",
      "Epoch 8/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.4664 - accuracy: 0.8825 - val_loss: 0.4598 - val_accuracy: 0.9183\n",
      "Epoch 9/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.4360 - accuracy: 0.8929 - val_loss: 0.4483 - val_accuracy: 0.9205\n",
      "Epoch 10/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.4161 - accuracy: 0.8986 - val_loss: 0.4538 - val_accuracy: 0.9181\n",
      "Epoch 11/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3999 - accuracy: 0.9042 - val_loss: 0.4454 - val_accuracy: 0.9198\n",
      "Epoch 12/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3854 - accuracy: 0.9060 - val_loss: 0.4503 - val_accuracy: 0.9190\n",
      "Epoch 13/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3754 - accuracy: 0.9099 - val_loss: 0.4353 - val_accuracy: 0.9272\n",
      "Epoch 14/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3644 - accuracy: 0.9123 - val_loss: 0.4302 - val_accuracy: 0.9269\n",
      "Epoch 15/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3518 - accuracy: 0.9182 - val_loss: 0.4294 - val_accuracy: 0.9261\n",
      "Epoch 16/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.3431 - accuracy: 0.9180 - val_loss: 0.4362 - val_accuracy: 0.9223\n",
      "Epoch 17/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3325 - accuracy: 0.9212 - val_loss: 0.4271 - val_accuracy: 0.9277\n",
      "Epoch 18/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3249 - accuracy: 0.9257 - val_loss: 0.4299 - val_accuracy: 0.9251\n",
      "Epoch 19/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3153 - accuracy: 0.9260 - val_loss: 0.4331 - val_accuracy: 0.9214\n",
      "Epoch 20/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3127 - accuracy: 0.9256 - val_loss: 0.4290 - val_accuracy: 0.9244\n",
      "Epoch 21/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.3040 - accuracy: 0.9284 - val_loss: 0.4321 - val_accuracy: 0.9253\n",
      "Epoch 22/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.3020 - accuracy: 0.9302 - val_loss: 0.4288 - val_accuracy: 0.9261\n",
      "Epoch 23/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2939 - accuracy: 0.9321 - val_loss: 0.4300 - val_accuracy: 0.9209\n",
      "Epoch 24/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2899 - accuracy: 0.9327 - val_loss: 0.4278 - val_accuracy: 0.9272\n",
      "Epoch 25/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2881 - accuracy: 0.9332 - val_loss: 0.4295 - val_accuracy: 0.9266\n",
      "Epoch 26/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2842 - accuracy: 0.9353 - val_loss: 0.4252 - val_accuracy: 0.9238\n",
      "Epoch 27/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2782 - accuracy: 0.9369 - val_loss: 0.4163 - val_accuracy: 0.9291\n",
      "Epoch 28/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2789 - accuracy: 0.9356 - val_loss: 0.4204 - val_accuracy: 0.9249\n",
      "Epoch 29/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2698 - accuracy: 0.9381 - val_loss: 0.4161 - val_accuracy: 0.9296\n",
      "Epoch 30/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2660 - accuracy: 0.9362 - val_loss: 0.4112 - val_accuracy: 0.9317\n",
      "Epoch 31/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2651 - accuracy: 0.9379 - val_loss: 0.4120 - val_accuracy: 0.9277\n",
      "Epoch 32/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2590 - accuracy: 0.9402 - val_loss: 0.4214 - val_accuracy: 0.9246\n",
      "Epoch 33/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2561 - accuracy: 0.9407 - val_loss: 0.4086 - val_accuracy: 0.9281\n",
      "Epoch 34/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2579 - accuracy: 0.9390 - val_loss: 0.4175 - val_accuracy: 0.9219\n",
      "Epoch 35/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.2490 - accuracy: 0.9433 - val_loss: 0.4228 - val_accuracy: 0.9261\n",
      "Epoch 36/2000\n",
      "18102/18102 [==============================] - 1s 35us/sample - loss: 0.2500 - accuracy: 0.9412 - val_loss: 0.4213 - val_accuracy: 0.9251\n",
      "Epoch 37/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2470 - accuracy: 0.9421 - val_loss: 0.4185 - val_accuracy: 0.9261\n",
      "Epoch 38/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2403 - accuracy: 0.9440 - val_loss: 0.4075 - val_accuracy: 0.9297\n",
      "Epoch 39/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2450 - accuracy: 0.9403 - val_loss: 0.4043 - val_accuracy: 0.9271\n",
      "Epoch 40/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2432 - accuracy: 0.9432 - val_loss: 0.3986 - val_accuracy: 0.9299\n",
      "Epoch 41/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2373 - accuracy: 0.9443 - val_loss: 0.4076 - val_accuracy: 0.9241\n",
      "Epoch 42/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2339 - accuracy: 0.9454 - val_loss: 0.4001 - val_accuracy: 0.9324\n",
      "Epoch 43/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2332 - accuracy: 0.9450 - val_loss: 0.4003 - val_accuracy: 0.9279\n",
      "Epoch 44/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2276 - accuracy: 0.9476 - val_loss: 0.4025 - val_accuracy: 0.9276\n",
      "Epoch 45/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2271 - accuracy: 0.9472 - val_loss: 0.4039 - val_accuracy: 0.9271\n",
      "Epoch 46/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2246 - accuracy: 0.9475 - val_loss: 0.4162 - val_accuracy: 0.9216\n",
      "Epoch 47/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2271 - accuracy: 0.9466 - val_loss: 0.3958 - val_accuracy: 0.9316\n",
      "Epoch 48/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2178 - accuracy: 0.9504 - val_loss: 0.3994 - val_accuracy: 0.9234\n",
      "Epoch 49/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2180 - accuracy: 0.9493 - val_loss: 0.3928 - val_accuracy: 0.9297\n",
      "Epoch 50/2000\n",
      "18102/18102 [==============================] - 1s 28us/sample - loss: 0.2184 - accuracy: 0.9500 - val_loss: 0.3921 - val_accuracy: 0.9316\n",
      "Epoch 51/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2170 - accuracy: 0.9492 - val_loss: 0.3904 - val_accuracy: 0.9309\n",
      "Epoch 52/2000\n",
      "18102/18102 [==============================] - 1s 29us/sample - loss: 0.2154 - accuracy: 0.9489 - val_loss: 0.3893 - val_accuracy: 0.9312\n",
      "Epoch 53/2000\n",
      "18102/18102 [==============================] - 1s 30us/sample - loss: 0.2121 - accuracy: 0.9520 - val_loss: 0.4060 - val_accuracy: 0.9234\n",
      "Epoch 54/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.2092 - accuracy: 0.9510 - val_loss: 0.3913 - val_accuracy: 0.9302\n",
      "Epoch 55/2000\n",
      "18102/18102 [==============================] - 1s 35us/sample - loss: 0.2064 - accuracy: 0.9512 - val_loss: 0.3816 - val_accuracy: 0.9304\n",
      "Epoch 56/2000\n",
      "18102/18102 [==============================] - 1s 34us/sample - loss: 0.2037 - accuracy: 0.9524 - val_loss: 0.3929 - val_accuracy: 0.9289\n",
      "Epoch 57/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.2075 - accuracy: 0.9509 - val_loss: 0.3810 - val_accuracy: 0.9324\n",
      "Epoch 58/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.2045 - accuracy: 0.9532 - val_loss: 0.3985 - val_accuracy: 0.9244\n",
      "Epoch 59/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.2007 - accuracy: 0.9529 - val_loss: 0.3820 - val_accuracy: 0.9282\n",
      "Epoch 60/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1982 - accuracy: 0.9532 - val_loss: 0.3818 - val_accuracy: 0.9292\n",
      "Epoch 61/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.2006 - accuracy: 0.9525 - val_loss: 0.3938 - val_accuracy: 0.9249\n",
      "Epoch 62/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1972 - accuracy: 0.9534 - val_loss: 0.3827 - val_accuracy: 0.9287\n",
      "Epoch 63/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.2018 - accuracy: 0.9502 - val_loss: 0.3776 - val_accuracy: 0.9330\n",
      "Epoch 64/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1922 - accuracy: 0.9549 - val_loss: 0.3780 - val_accuracy: 0.9322\n",
      "Epoch 65/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1963 - accuracy: 0.9523 - val_loss: 0.3729 - val_accuracy: 0.9296\n",
      "Epoch 66/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1955 - accuracy: 0.9528 - val_loss: 0.3666 - val_accuracy: 0.9322\n",
      "Epoch 67/2000\n",
      "18102/18102 [==============================] - 1s 33us/sample - loss: 0.1910 - accuracy: 0.9541 - val_loss: 0.3714 - val_accuracy: 0.9312\n",
      "Epoch 68/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1902 - accuracy: 0.9533 - val_loss: 0.3785 - val_accuracy: 0.9316\n",
      "Epoch 69/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1918 - accuracy: 0.9540 - val_loss: 0.3731 - val_accuracy: 0.9302\n",
      "Epoch 70/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.1868 - accuracy: 0.9554 - val_loss: 0.3716 - val_accuracy: 0.9325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1885 - accuracy: 0.9531 - val_loss: 0.3711 - val_accuracy: 0.9291\n",
      "Epoch 72/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1852 - accuracy: 0.9559 - val_loss: 0.3646 - val_accuracy: 0.9312\n",
      "Epoch 73/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1877 - accuracy: 0.9552 - val_loss: 0.3627 - val_accuracy: 0.9335\n",
      "Epoch 74/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1842 - accuracy: 0.9572 - val_loss: 0.3747 - val_accuracy: 0.9296\n",
      "Epoch 75/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1857 - accuracy: 0.9550 - val_loss: 0.3909 - val_accuracy: 0.9248\n",
      "Epoch 76/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1822 - accuracy: 0.9583 - val_loss: 0.3723 - val_accuracy: 0.9321\n",
      "Epoch 77/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1846 - accuracy: 0.9540 - val_loss: 0.3679 - val_accuracy: 0.9330\n",
      "Epoch 78/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1773 - accuracy: 0.9581 - val_loss: 0.3756 - val_accuracy: 0.9316\n",
      "Epoch 79/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1784 - accuracy: 0.9570 - val_loss: 0.3917 - val_accuracy: 0.9238\n",
      "Epoch 80/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1810 - accuracy: 0.9568 - val_loss: 0.3750 - val_accuracy: 0.9296\n",
      "Epoch 81/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1789 - accuracy: 0.9557 - val_loss: 0.3793 - val_accuracy: 0.9287\n",
      "Epoch 82/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1766 - accuracy: 0.9585 - val_loss: 0.3773 - val_accuracy: 0.9277\n",
      "Epoch 83/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1767 - accuracy: 0.9573 - val_loss: 0.3510 - val_accuracy: 0.9339\n",
      "Epoch 84/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1783 - accuracy: 0.9545 - val_loss: 0.3612 - val_accuracy: 0.9307\n",
      "Epoch 85/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1739 - accuracy: 0.9588 - val_loss: 0.3726 - val_accuracy: 0.9251\n",
      "Epoch 86/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1778 - accuracy: 0.9553 - val_loss: 0.3663 - val_accuracy: 0.9277\n",
      "Epoch 87/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1742 - accuracy: 0.9579 - val_loss: 0.3596 - val_accuracy: 0.9354\n",
      "Epoch 88/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1720 - accuracy: 0.9584 - val_loss: 0.3604 - val_accuracy: 0.9272\n",
      "Epoch 89/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1738 - accuracy: 0.9590 - val_loss: 0.3681 - val_accuracy: 0.9281\n",
      "Epoch 90/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1732 - accuracy: 0.9570 - val_loss: 0.3675 - val_accuracy: 0.9289\n",
      "Epoch 91/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1712 - accuracy: 0.9567 - val_loss: 0.3613 - val_accuracy: 0.9311\n",
      "Epoch 92/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1729 - accuracy: 0.9565 - val_loss: 0.3595 - val_accuracy: 0.9314\n",
      "Epoch 93/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1750 - accuracy: 0.9560 - val_loss: 0.3707 - val_accuracy: 0.9312\n",
      "Epoch 94/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1696 - accuracy: 0.9591 - val_loss: 0.3664 - val_accuracy: 0.9311\n",
      "Epoch 95/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1702 - accuracy: 0.9576 - val_loss: 0.3665 - val_accuracy: 0.9316\n",
      "Epoch 96/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1674 - accuracy: 0.9581 - val_loss: 0.3600 - val_accuracy: 0.9325\n",
      "Epoch 97/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1700 - accuracy: 0.9581 - val_loss: 0.3736 - val_accuracy: 0.9258\n",
      "Epoch 98/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1662 - accuracy: 0.9607 - val_loss: 0.3640 - val_accuracy: 0.9299\n",
      "Epoch 99/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1637 - accuracy: 0.9588 - val_loss: 0.3682 - val_accuracy: 0.9302\n",
      "Epoch 100/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1632 - accuracy: 0.9590 - val_loss: 0.3576 - val_accuracy: 0.9316\n",
      "Epoch 101/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1679 - accuracy: 0.9586 - val_loss: 0.3577 - val_accuracy: 0.9316\n",
      "Epoch 102/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1628 - accuracy: 0.9612 - val_loss: 0.3828 - val_accuracy: 0.9234\n",
      "Epoch 103/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1619 - accuracy: 0.9604 - val_loss: 0.3640 - val_accuracy: 0.9287\n",
      "Epoch 104/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1644 - accuracy: 0.9589 - val_loss: 0.3459 - val_accuracy: 0.9340\n",
      "Epoch 105/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1606 - accuracy: 0.9609 - val_loss: 0.3754 - val_accuracy: 0.9263\n",
      "Epoch 106/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1623 - accuracy: 0.9583 - val_loss: 0.3776 - val_accuracy: 0.9243\n",
      "Epoch 107/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1622 - accuracy: 0.9591 - val_loss: 0.3620 - val_accuracy: 0.9299\n",
      "Epoch 108/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1588 - accuracy: 0.9613 - val_loss: 0.3576 - val_accuracy: 0.9329\n",
      "Epoch 109/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1629 - accuracy: 0.9576 - val_loss: 0.3769 - val_accuracy: 0.9267\n",
      "Epoch 110/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1580 - accuracy: 0.9607 - val_loss: 0.3756 - val_accuracy: 0.9248\n",
      "Epoch 111/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1590 - accuracy: 0.9607 - val_loss: 0.3853 - val_accuracy: 0.9231\n",
      "Epoch 112/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1571 - accuracy: 0.9615 - val_loss: 0.3589 - val_accuracy: 0.9307\n",
      "Epoch 113/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1556 - accuracy: 0.9613 - val_loss: 0.3380 - val_accuracy: 0.9364\n",
      "Epoch 114/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1582 - accuracy: 0.9598 - val_loss: 0.3541 - val_accuracy: 0.9317\n",
      "Epoch 115/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1588 - accuracy: 0.9585 - val_loss: 0.3795 - val_accuracy: 0.9221\n",
      "Epoch 116/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1532 - accuracy: 0.9622 - val_loss: 0.3809 - val_accuracy: 0.9266\n",
      "Epoch 117/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1507 - accuracy: 0.9638 - val_loss: 0.3602 - val_accuracy: 0.9297\n",
      "Epoch 118/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1577 - accuracy: 0.9608 - val_loss: 0.3680 - val_accuracy: 0.9297\n",
      "Epoch 119/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1576 - accuracy: 0.9608 - val_loss: 0.3782 - val_accuracy: 0.9251\n",
      "Epoch 120/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1592 - accuracy: 0.9599 - val_loss: 0.3453 - val_accuracy: 0.9327\n",
      "Epoch 121/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1527 - accuracy: 0.9612 - val_loss: 0.3603 - val_accuracy: 0.9272\n",
      "Epoch 122/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1559 - accuracy: 0.9599 - val_loss: 0.3684 - val_accuracy: 0.9269\n",
      "Epoch 123/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1537 - accuracy: 0.9611 - val_loss: 0.3977 - val_accuracy: 0.9180\n",
      "Epoch 124/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1500 - accuracy: 0.9635 - val_loss: 0.3562 - val_accuracy: 0.9340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1524 - accuracy: 0.9606 - val_loss: 0.3707 - val_accuracy: 0.9272\n",
      "Epoch 126/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1499 - accuracy: 0.9624 - val_loss: 0.3658 - val_accuracy: 0.9281\n",
      "Epoch 127/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1540 - accuracy: 0.9614 - val_loss: 0.3442 - val_accuracy: 0.9332\n",
      "Epoch 128/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1530 - accuracy: 0.9601 - val_loss: 0.3541 - val_accuracy: 0.9307\n",
      "Epoch 129/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1547 - accuracy: 0.9604 - val_loss: 0.3519 - val_accuracy: 0.9350\n",
      "Epoch 130/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1504 - accuracy: 0.9623 - val_loss: 0.3546 - val_accuracy: 0.9286\n",
      "Epoch 131/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1476 - accuracy: 0.9630 - val_loss: 0.3658 - val_accuracy: 0.9269\n",
      "Epoch 132/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1503 - accuracy: 0.9624 - val_loss: 0.3727 - val_accuracy: 0.9263\n",
      "Epoch 133/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1489 - accuracy: 0.9631 - val_loss: 0.3510 - val_accuracy: 0.9294\n",
      "Epoch 134/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1519 - accuracy: 0.9613 - val_loss: 0.3471 - val_accuracy: 0.9322\n",
      "Epoch 135/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1500 - accuracy: 0.9622 - val_loss: 0.3560 - val_accuracy: 0.9272\n",
      "Epoch 136/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1497 - accuracy: 0.9609 - val_loss: 0.3712 - val_accuracy: 0.9228\n",
      "Epoch 137/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1531 - accuracy: 0.9603 - val_loss: 0.3396 - val_accuracy: 0.9335\n",
      "Epoch 138/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1475 - accuracy: 0.9626 - val_loss: 0.3864 - val_accuracy: 0.9218\n",
      "Epoch 139/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1471 - accuracy: 0.9620 - val_loss: 0.3609 - val_accuracy: 0.9267\n",
      "Epoch 140/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1477 - accuracy: 0.9622 - val_loss: 0.3705 - val_accuracy: 0.9256\n",
      "Epoch 141/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1477 - accuracy: 0.9624 - val_loss: 0.3501 - val_accuracy: 0.9309\n",
      "Epoch 142/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1402 - accuracy: 0.9638 - val_loss: 0.3620 - val_accuracy: 0.9287\n",
      "Epoch 143/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1467 - accuracy: 0.9627 - val_loss: 0.3564 - val_accuracy: 0.9294\n",
      "Epoch 144/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1425 - accuracy: 0.9633 - val_loss: 0.3572 - val_accuracy: 0.9297\n",
      "Epoch 145/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.1437 - accuracy: 0.9650 - val_loss: 0.3471 - val_accuracy: 0.9324\n",
      "Epoch 146/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1480 - accuracy: 0.9616 - val_loss: 0.3577 - val_accuracy: 0.9307\n",
      "Epoch 147/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1421 - accuracy: 0.9626 - val_loss: 0.3566 - val_accuracy: 0.9316\n",
      "Epoch 148/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1444 - accuracy: 0.9627 - val_loss: 0.3501 - val_accuracy: 0.9306\n",
      "Epoch 149/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1433 - accuracy: 0.9622 - val_loss: 0.3581 - val_accuracy: 0.9297\n",
      "Epoch 150/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1447 - accuracy: 0.9620 - val_loss: 0.3584 - val_accuracy: 0.9282\n",
      "Epoch 151/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1450 - accuracy: 0.9623 - val_loss: 0.3778 - val_accuracy: 0.9208\n",
      "Epoch 152/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1432 - accuracy: 0.9632 - val_loss: 0.3668 - val_accuracy: 0.9264\n",
      "Epoch 153/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1407 - accuracy: 0.9623 - val_loss: 0.3730 - val_accuracy: 0.9272\n",
      "Epoch 154/2000\n",
      "18102/18102 [==============================] - 1s 32us/sample - loss: 0.1460 - accuracy: 0.9620 - val_loss: 0.3616 - val_accuracy: 0.9281\n",
      "Epoch 155/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1387 - accuracy: 0.9659 - val_loss: 0.3542 - val_accuracy: 0.9304\n",
      "Epoch 156/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1400 - accuracy: 0.9647 - val_loss: 0.3593 - val_accuracy: 0.9277\n",
      "Epoch 157/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1452 - accuracy: 0.9608 - val_loss: 0.3553 - val_accuracy: 0.9325\n",
      "Epoch 158/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1407 - accuracy: 0.9628 - val_loss: 0.3428 - val_accuracy: 0.9342\n",
      "Epoch 159/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1455 - accuracy: 0.9617 - val_loss: 0.3530 - val_accuracy: 0.9319\n",
      "Epoch 160/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1412 - accuracy: 0.9640 - val_loss: 0.3741 - val_accuracy: 0.9231\n",
      "Epoch 161/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1385 - accuracy: 0.9640 - val_loss: 0.3621 - val_accuracy: 0.9292\n",
      "Epoch 162/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1379 - accuracy: 0.9629 - val_loss: 0.3861 - val_accuracy: 0.9274\n",
      "Epoch 163/2000\n",
      "18102/18102 [==============================] - 1s 31us/sample - loss: 0.1421 - accuracy: 0.9633 - val_loss: 0.3682 - val_accuracy: 0.9286\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for f in range(1, 11, 1):\n",
    "    frac = f / 10.0\n",
    "    sample = data.sample(frac=frac, random_state=12345)\n",
    "    train, validation, test = train_validation_test_split(sample)\n",
    "\n",
    "    train.sort_values(\"time\", inplace=True)\n",
    "    validation.sort_values(\"time\", inplace=True)\n",
    "    test.sort_values(\"time\", inplace=True)\n",
    "\n",
    "    train_rolled = train.groupby(\"location\").rolling(15, min_periods=1).mean().reset_index()\n",
    "    validation_rolled = validation.groupby(\"location\").rolling(15, min_periods=1).mean().reset_index()\n",
    "    test_rolled = test.groupby(\"location\").rolling(15, min_periods=1).mean().reset_index()\n",
    "\n",
    "    train_imputed = train.set_index(\"location\").groupby(\"location\").ffill()\n",
    "    train_imputed.fillna(0, inplace=True)\n",
    "    train_imputed.reset_index(inplace=True)\n",
    "\n",
    "    validation_imputed = validation.set_index(\"location\").groupby(\"location\").ffill()\n",
    "    validation_imputed.fillna(0, inplace=True)\n",
    "    validation_imputed.reset_index(inplace=True)\n",
    "\n",
    "    test_imputed = test.set_index(\"location\").groupby(\"location\").ffill()\n",
    "    test_imputed.fillna(0, inplace=True)\n",
    "    test_imputed.reset_index(inplace=True)\n",
    "\n",
    "    X_train, y_train = train_imputed[scanners].values, train_imputed[\"location\"].values\n",
    "    X_validation, y_validation = validation_imputed[scanners].values, validation_imputed[\"location\"].values\n",
    "    X_test, y_test = test_imputed[scanners].values, test_imputed[\"location\"].values\n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    y_train = enc.fit_transform(y_train)\n",
    "    y_validation = enc.transform(y_validation)\n",
    "    y_test = enc.transform(y_test)\n",
    "\n",
    "    model = MLPClassifier(size='small')\n",
    "    history = model.fit(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "    train_loss.append(history.history['loss'][-1])\n",
    "    val_loss.append(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.1, 1.1, 0.1)\n",
    "plt.plot(x, train_loss, label='Training loss')\n",
    "plt.plot(x, val_loss, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Dataset size\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Models/MLP_Classification_Rolling_FFill_MinMax.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
